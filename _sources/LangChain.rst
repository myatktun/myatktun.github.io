=========
LangChain
=========

1. `Basics`_
2. `Data Manipulation`_
3. `Prompting Basics`_

`back to top <#langchain>`_

Basics
======

* `Interfaces`_, `LLM Output`_, `Component Composition`_
* LangChain provides abstractions for each major prompting technique, utilising Python and
  JavaScript for wrappers
* has integrations with commercial and open source LLM providers
* prompt templates enable to reuse prompts more than once, and store them in the LangChain Hub

Interfaces
----------
    * **Chat Model**
        - LLM providers like OpenAI differentiate messages sent to and from the model into
          roles
        - System role: for instructions the model should use to answer a user question
        - User role: for user's query and other content produced by the user
        - Assistant role: for content generated by the model
        - ``temperature``: controls sampling algorithm, lower values produce more predictable
          outputs, and higher values do better for creative tasks
        - ``max_tokens``: limits the size and cost of output
        - chat models make use of different types of chat message interfaces associated with
          each role
        - ``HumanMessage``: message sent from human, user role
        - ``AIMessage``: message sent from AI, assistant role
        - ``SystemMessage``: message setting the instructions for AI, system role
        - ``ChatMessage``: message for arbitrary setting of role
    * **LLM**
        - takes a string input, send it to the model provider, and returns the model
          prediction as output
        - LangChain interact with LLMs using function calling or traditional prompting
    * **Prompt Template**
        - allow to construct prompts with dynamic inputs
        - use ``ChatPromptTemplate`` for AI chat applications

        .. code-block:: python

           from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
   
           template = PromptTemplate.from_template("""Answer the question based on the context
           below.
   
           Context: {context}
           Question: {question}
           Answer: """)
   
           # Using ChatPromptTemplate will associate with roles
           template = ChatPromptTemplate.from_messages([
               ('system', 'Answer the question base on the context below.'),
               ('human', 'Context: {context}'),
               ('human', 'Question: {question}')
           ])
   
           prompt = template.invoke({
               "context": "This is context",
               "question": "Question based on context?"
           })


    * **Runnable**
        - ``invoke()``: single input to output
        - ``batch()``:  multiple inputs to multiple outputs
        - ``stream()``: stream output from a single input as it's produced
        - each method has ``asyncio`` equivalents
        - utilities for retries, fallbacks, schemas, and runtime configurability are available

        .. code-block:: python

           model.invoke('Hi there!')
   
           model.batch(['Hi there!', 'Bye!'])
   
           for token in model.stream('Bye!'):
               print(token)



LLM Output
----------
    * LLM can generate specific output format, such as JSON, XML, CSV
    * **JSON Output**
        - need to define schema using Pydantic, and include it in the prompt
        - schema is converted to ``JSONSchema`` object, and used to validate the output from LLM

        .. code-block:: python

           class AnswerWithJustification(BaseModel):
               '''An answer to the user's question along with justification for the answer. '''
               answer: str
               justification: str
   
           llm = ChatOpenAI(model="supported_model", temperature=0)
   
           structured_llm = llm.with_structured_output(AnswerWithJustification)
           msg = structured_llm.invoke("""Question""")


    * **Output Parsers**
        - classes to structure LLM responses
        - can be used to provide output format instructions in the prompt
        - textual output can be rendered to a more structured format

        .. code-block:: python

           from langchain_core.output_parsers import CommaSeparatedListOutputParser
   
           parser = CommaSeparatedListOutputParser()
           items = parser.invoke("apple, banana, cherry") # ['apple', 'banana', 'cherry']



Component Composition
---------------------
    * **Imperative Composition**
        - calling components directly, e.g. ``model.invoke()``
        - Parallel execution: threads or coroutines in Python, and ``Promise.all`` in JavaScript
        - Streaming: using ``yield``
        - Async execution: with async functions
        - useful for writing custom logic

        .. code-block:: python

           template = ChatPromptTemplate.from_messages([
               ('system', 'You are a helpful assistant.'),
               ('human', '{question}')
           ])
   
           model = ChatOpenAI(model="gpt-3.5-turbo")
   
           # combine components in a function
           @chain
           def chatbot(values):
               prompt = template.invoke(values)
               for token in model.stream(prompt):
                   yield token
   
           for part in chatbot.stream({"question": "Question"}):
               print(part.content, end=' ')


    * **Declarative Composition**
        - using LCEL (LangChain Expression Language)
        - LCEL compositions are compiled to an optimised execution plan
        - Streaming, Parallel and Async executions are automatic
        - useful for assembling existing components with limited customisation

        .. code-block:: python

           template = ChatPromptTemplate.from_messages([
               ('system', 'You are a helpful assistant.'),
               ('human', '{question}')
           ])
   
           model = ChatOpenAI(model="gpt-3.5-turbo")
   
           # combine components with | operator
           chatbot = template | model
   
           for part in chatbot.stream({"question": "Question"}):
               print(part.content, end=' ')


`back to top <#langchain>`_

Data Manipulation
=================

* `Data Indexing`_, `Indexing Optimisations`_
* Retrieving: get data from the index, and use as context for the LLM

Data Indexing
-------------
    * preprocessing data so application can find the most relevant ones for each question
    * Ingestion: converting documents into embeddings, and storing in vector store
    * Context Window: size of input and output tokens LLMs and embedding models can handle
    * **Document Loader**
        - can load files such as txt, csv, json, Markdown, and integrate with platforms such
          as Slack and Notion
        - can use ``WebBaseLoader`` to load HTML, or ``PyPDFLoader`` with ``pypdf`` package
        - loaded data is stored in ``Document`` class, and need to be split into chunks
          semantically
    * ``RecursiveCharacterTextSplitter``
        - can split text based on a list of separators in order
        - default separator order: ``\n\n`` (paragraph), ``\n`` (line), space (word)
        - split paragraphs that are within the chunk size
        - for paragraphs longer than the chunk size, split by the next separator
        - each chunk is a ``Document`` with metadata of the original document
        - can use for others, such as code languages and Markdown, with relevant separators

        .. code-block:: python

           from langchain_text_splitters import Language, RecursiveCharacterTextSplitter
           from langchain_community.document_loaders import TextLoader
   
           loader = TextLoader("./main.py")
           docs = loader.load()
   
           splitter = RecursiveCharacterTextSplitter.from_language(
               language=Language.PYTHON,
               chunk_size=50,
               chunk_overlap=0
           )
   
           python_docs = splitter.split_documents(docs)


    * **Embedding**
        - converting text to numbers that cannot be used to recover original text
        - both text and numerals are stored since it is a lossy representation
        - Dense embeddings: low-dimensional vectors with mostly non-zero values
        - Sparse embeddings: high-dimensional vectors with mostly zero values
        - never combine embeddings from different models
        - words or sentences that are close in meaning should be closer in semantic dimension
        - cosine similarity is usually used for degree of similarity
        - ``Embeddings`` class interfaces with text embedding models, and generate vector
          representations
        - can embed documents and query
        - embedding multiple documents at the same time is more efficient

        .. code-block:: python

           from langchain_openai import OpenAIEmbeddings
   
           model = OpenAIEmbeddings()
   
           embeddings = model.embed_documents([
               "Hi there!",
               "Oh, hello!",
               "What's your name?",
               "My friends call me World",
               "Hello World!"
           ])


    * **Vector Store**
        - database to store vectors and perform complex calculations
        - handle unstructured data, including text and images
        - has capabilities such as multi-tenancy and metadata filtering
        - PostgreSQL can be used as vector store with ``pgvector`` extension
        - ``add_documents()``: create embeddings for each document, and store them

        .. code-block:: python

           connection = 'PostgreSQL_Connection'
   
           db = PGVector.from_documents(docs, embeddings_model, connection=connection)
   
           db.similarity_search("query", k=N)
   
           db.add_documents(
               [
                   Document(
                       page_content="Content",
                       metadata={"key": "value"}
                   )
               ],
               ids=ids
           )
   
           db.delete(ids=['1'])


    * **Indexing API**
        - uses ``RecordManager`` to track document writes into the vector store
        - stores document hash, write time, and source ID
        - provides cleanup modes to delete existing documents
        - ``None``: manual clean up of old content
        - ``Icremental`` & ``full``: delete previous versions if content of the source document or
          derived ones change
        - ``Full``: delete any documents not included in documents currently being indexed

        .. code-block:: python

           from langchain.indexes import SQLRecordManager, index
   
           collection_name = "my_docs"
           embeddings_model = OpenAIEmbeddings(model="text-embedding-3-small")
           namespace = "my_docs_namespace"
   
           vectorstore = PGVector(
               embeddings=embeddings_model,
               collection_name=collection_name,
               connection=connection,
               use_jsonb=True
           )
   
           record_manager = SQLRecordManager(
               namespace,
               db_url=connection
           )
   
           record_manager.create_schema()
   
           docs = [
               Document(
                   page_content="content 1",
                   metadata={"id": 1, "source": "source_1.txt"}
               ),
               Document(
                   page_content="content 2",
                   metadata={"id": 2, "source": "source_2.txt"}
               )
           ]
   
           index_1 = index(
               docs,
               record_manager,
               vectorstore,
               cleanup="incremental",
               source_id_key="source"
           )
   
           print("Index attempt 1: ", index_1)
   
           index_2 = index(
               docs,
               record_manager,
               vectorstore,
               cleanup="incremental",
               source_id_key="source"
           )
   
           # attempting to index again will not add the documents
           print("Index attempt 2: ", index_2)
   
           docs[0].page_content = "modified"
   
           index_3 = index(
               docs,
               record_manager,
               vectorstore,
               cleanup="incremental",
               source_id_key="source"
           )
   
           # new version is written, and all old versions sharing the same source are deleted
           print("Index attempt 3: ", index_3)



Indexing Optimisations
----------------------
    * ``MultiVectorRetriever``
        - decouple documents to use for answer synthesis
        - e.g. in a document of text and tables, embed summaries of table elements with an id
          reference to the full raw table, which is stored in a separate Docstore
        - enables to provide the model with full context to answer user's question

        .. code-block:: python

           from langchain_core.output_parsers import StrOutputParser
           from langchain_core.prompts import ChatPromptTemplate
           from langchain.retrievers.multi_vector import MultiVectorRetriever
           from langchain.storage import InMemoryStore
   
           prompt_text = "Summarize the following document:\n\n{doc}"
   
           prompt = ChatPromptTemplate.from_template(prompt_text)
   
           llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo")
   
           summarize_chain = {
               "doc": lambda x: x.page_content} | prompt | llm | StrOutputParser()
   
           summaries = summarize_chain.batch(chunks, {"max_concurrency": 5})
   
           vectorstore = PGVector(
               embeddings=embeddings_model,
               collection_name=collection_name,
               connection=connection,
               use_jsonb=True
           )
   
           # for parent documents
           store = InMemoryStore()
           id_key = "doc_id"
   
           retriever = MultiVectorRetriever(
               vectorstore=vectorstore,
               docstore=store,
               id_key=id_key
           )
   
           doc_ids = [str(uuid.uuid4()) for _ in chunks]
   
           summary_docs = [
               Document(page_content=s, metadata={id_key: doc_ids[i]})
               for i, s in enumerate(summaries)
           ]
   
           retriever.vectorstore.add_documents(summary_docs)
   
           # store the original documents, linked to summaries via doc_ids
           retriever.docstore.mset(list(zip(doc_ids, chunks)))
   
           # vector store retrieves the summaries
           sub_docs = retriever.vectorstore.similarity_search(
               "chapter on philosophy", k=2)
   
           # retriever return the larger source document chunks
           retrieved_docs = retriever.invoke("chapter on philosophy")


`back to top <#langchain>`_

Prompting Basics
================

* `LLMs`_, `Zero-Shot Prompting`_, `Few-Shot Prompting`_
* prompts help the model understand context and generate relevant answers to queries
* prompt engineering: adapting an existing LLM for specific task
* Temperature: controls the randomness of LLM output
* prompting techniques are most useful when combined with others


LLMs
----
    * **Fine-Tuned**
        - created by taking base LLMs, and further train on a proprietary dataset for a
          specific task
    * **Instruction-Tuned**
        - fine-tuned with task-specific datasets and RLHF
    * **Dialogue-Tuned**
        - enhanced instruction-tuned LLMs
        - uses dialogue dataset and chat format
        - text is divided into parts associated with a role
        - System role: for instructions and framing the task
        - User role: actual task or question
        - Assistant role: for outputs of the model

Zero-Shot Prompting
-------------------
    * simply telling the LLM to perform the desired task
    * usually work for simple questions
    * will need to iterate on prompts and responses to get a reliable system
    * **Chain-of-Thought**
        - instructing the model to take time to think
        - prepending the prompt with instructions form the LLM to describe how it could arrive
          at the answer
    * **Retrieval-Augmented Generation**
        - RAG: finding relevant context, and including them in the prompt
        - should be combined with CoT
    * **Tool Calling**
        - prepending the prompt with a list of external functions LLM can use
        - developer should parse the output, and call functions that the LLM wants to use

Few-Shot Prompting
------------------
    * providing LLM with examples of other questions and correct answers
    * enables LLM to learn how to perform a new task without going through additional training
      or fine-tuning
    * less powerful than fine-tuning, but more flexible and can do it at query time
    * **Static**
        - include a predetermined list of a small number of examples in the prompt
    * **Dynamic**
        - from a dataset of many examples, choose the most relevant ones for each new query

`back to top <#langchain>`_
