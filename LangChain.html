
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>LangChain &#8212; Notes</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'LangChain';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Linux" href="Linux.html" />
    <link rel="prev" title="Kubernetes" href="Kubernetes.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Notes</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="AITools.html">AI Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="AWS.html">AWS</a></li>
<li class="toctree-l1"><a class="reference internal" href="ArmArchitecture.html">Arm Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="Assembly.html">Assembly</a></li>
<li class="toctree-l1"><a class="reference internal" href="BackendEngineering.html">Backend Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="C.html">C</a></li>
<li class="toctree-l1"><a class="reference internal" href="C%2B%2B.html">C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="CSharp.html">C# #</a></li>
<li class="toctree-l1"><a class="reference internal" href="CTF.html">CTF</a></li>
<li class="toctree-l1"><a class="reference internal" href="CompTIA.html">CompTIA Cert Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="Compilers.html">Compilers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ComputerGraphics.html">Computer Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="ComputerVision.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="ComputingSystems.html">Computing Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="CyberSecurity.html">Cyber Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="DataStructure%26Algorithms.html">Data Structure &amp; Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="DatabaseEngineering.html">Database Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="DeepLearning.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="DesignPatterns.html">Design Patterns</a></li>
<li class="toctree-l1"><a class="reference internal" href="Docker.html">Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="ECS.html">ECS</a></li>
<li class="toctree-l1"><a class="reference internal" href="FFmpeg.html">FFmpeg</a></li>
<li class="toctree-l1"><a class="reference internal" href="Git.html">Git</a></li>
<li class="toctree-l1"><a class="reference internal" href="Helm.html">Helm</a></li>
<li class="toctree-l1"><a class="reference internal" href="Istio.html">Istio</a></li>
<li class="toctree-l1"><a class="reference internal" href="JDBC.html">JDBC</a></li>
<li class="toctree-l1"><a class="reference internal" href="Java.html">Java</a></li>
<li class="toctree-l1"><a class="reference internal" href="Jenkins.html">Jenkins</a></li>
<li class="toctree-l1"><a class="reference internal" href="Kubernetes.html">Kubernetes</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="Linux.html">Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning.html">Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Microservices.html">Microservices</a></li>
<li class="toctree-l1"><a class="reference internal" href="MongoDB.html">MongoDB</a></li>
<li class="toctree-l1"><a class="reference internal" href="NetworkProgramming.html">Network Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="NodeJS.html">Node.js</a></li>
<li class="toctree-l1"><a class="reference internal" href="OpenAPI.html">OpenAPI</a></li>
<li class="toctree-l1"><a class="reference internal" href="OperatingSystem.html">Operating System</a></li>
<li class="toctree-l1"><a class="reference internal" href="POSIXThreads.html">POSIX Threads</a></li>
<li class="toctree-l1"><a class="reference internal" href="PenetrationTesting.html">Penetration Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="ProgramDesign.html">Program Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="ProjectManagement.html">Project Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="Prometheus.html">Prometheus</a></li>
<li class="toctree-l1"><a class="reference internal" href="Python.html">Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="React.html">React</a></li>
<li class="toctree-l1"><a class="reference internal" href="Redis.html">Redis</a></li>
<li class="toctree-l1"><a class="reference internal" href="SFML.html">SFML</a></li>
<li class="toctree-l1"><a class="reference internal" href="SQL.html">SQL</a></li>
<li class="toctree-l1"><a class="reference internal" href="StrategicPlanning.html">Strategic Planning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Terraform.html">Terraform</a></li>
<li class="toctree-l1"><a class="reference internal" href="TypeScript.html">TypeScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="UIUX.html">UI/UX</a></li>
<li class="toctree-l1"><a class="reference internal" href="Vagrant.html">Vagrant</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/LangChain.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LangChain</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basics">Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interfaces">Interfaces</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-output">LLM Output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#component-composition">Component Composition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag">RAG</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-indexing">Data Indexing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing-optimisations">Indexing Optimisations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-transformation">Query Transformation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-routing">Query Routing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-construction">Query Construction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#langgraph">LangGraph</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph">Graph</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory">Memory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiactor">Multiactor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chat-history">Chat History</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#subgraphs">Subgraphs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#langgraph-platform">LangGraph Platform</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-models">Data Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#features">Features</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cognitive-architectures">Cognitive Architectures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#degree-of-autonomy">Degree of Autonomy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-call-architecture">LLM Call Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-architecture">Chain Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#router-architecture">Router Architecture</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#agent-architecture">Agent Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-agent">Standard Agent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#always-tool-calling-first">Always Tool Calling First</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#managing-multiple-tools">Managing Multiple Tools</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reflection">Reflection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-agent">Multi-agent</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-patterns">LLM Patterns</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#structured-output">Structured Output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#streaming-output">Streaming Output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#human-in-the-loop">Human in the Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#double-texting-modes">Double Texting Modes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deployment">Deployment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompting-basics">Prompting Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llms">LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-prompting">Zero-Shot Prompting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#few-shot-prompting">Few-Shot Prompting</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="langchain">
<h1>LangChain<a class="headerlink" href="#langchain" title="Link to this heading">#</a></h1>
<ol class="arabic simple">
<li><p><a class="reference internal" href="#basics">Basics</a></p></li>
<li><p><a class="reference internal" href="#rag">RAG</a></p></li>
<li><p><a class="reference internal" href="#langgraph">LangGraph</a></p></li>
<li><p><a class="reference internal" href="#langgraph-platform">LangGraph Platform</a></p></li>
<li><p><a class="reference internal" href="#cognitive-architectures">Cognitive Architectures</a></p></li>
<li><p><a href="#id8"><span class="problematic" id="id9">`Agent Architectures`_</span></a></p></li>
<li><p><a class="reference internal" href="#llm-patterns">LLM Patterns</a></p></li>
<li><p><a class="reference internal" href="#deployment">Deployment</a></p></li>
<li><p><a class="reference internal" href="#prompting-basics">Prompting Basics</a></p></li>
</ol>
<p><a class="reference external" href="#langchain">back to top</a></p>
<section id="basics">
<h2>Basics<a class="headerlink" href="#basics" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#interfaces">Interfaces</a>, <a class="reference internal" href="#llm-output">LLM Output</a>, <a class="reference internal" href="#component-composition">Component Composition</a></p></li>
<li><p>LangChain provides abstractions for each major prompting technique, utilising Python and
JavaScript for wrappers</p></li>
<li><p>has integrations with commercial and open source LLM providers</p></li>
<li><p>prompt templates enable to reuse prompts more than once, and store them in the LangChain Hub</p></li>
</ul>
<section id="interfaces">
<h3>Interfaces<a class="headerlink" href="#interfaces" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><dl class="simple">
<dt><strong>Chat Model</strong></dt><dd><ul class="simple">
<li><p>LLM providers like OpenAI differentiate messages sent to and from the model into
roles</p></li>
<li><p>System role: for instructions the model should use to answer a user question</p></li>
<li><p>User role: for user’s query and other content produced by the user</p></li>
<li><p>Assistant role: for content generated by the model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">temperature</span></code>: controls sampling algorithm, lower values produce more predictable
outputs, and higher values do better for creative tasks</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_tokens</span></code>: limits the size and cost of output</p></li>
<li><p>chat models make use of different types of chat message interfaces associated with
each role</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HumanMessage</span></code>: message sent from human, user role</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">AIMessage</span></code>: message sent from AI, assistant role</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SystemMessage</span></code>: message setting the instructions for AI, system role</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ChatMessage</span></code>: message for arbitrary setting of role</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>LLM</strong></dt><dd><ul class="simple">
<li><p>takes a string input, send it to the model provider, and returns the model
prediction as output</p></li>
<li><p>LangChain interact with LLMs using function calling or traditional prompting</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Prompt Template</strong></dt><dd><ul class="simple">
<li><p>allow to construct prompts with dynamic inputs</p></li>
<li><p>use <code class="docutils literal notranslate"><span class="pre">ChatPromptTemplate</span></code> for AI chat applications</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span><span class="p">,</span> <span class="n">ChatPromptTemplate</span>

<span class="n">template</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;Answer the question based on the context</span>
<span class="s2">below.</span>

<span class="s2">Context: </span><span class="si">{context}</span>
<span class="s2">Question: </span><span class="si">{question}</span>
<span class="s2">Answer: &quot;&quot;&quot;</span><span class="p">)</span>

<span class="c1"># Using ChatPromptTemplate will associate with roles</span>
<span class="n">template</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;Answer the question base on the context below.&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;human&#39;</span><span class="p">,</span> <span class="s1">&#39;Context: </span><span class="si">{context}</span><span class="s1">&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;human&#39;</span><span class="p">,</span> <span class="s1">&#39;Question: </span><span class="si">{question}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">template</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span>
    <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="s2">&quot;This is context&quot;</span><span class="p">,</span>
    <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;Question based on context?&quot;</span>
<span class="p">})</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Runnable</strong></dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">invoke()</span></code>: single input to output</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch()</span></code>:  multiple inputs to multiple outputs</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stream()</span></code>: stream output from a single input as it’s produced</p></li>
<li><p>each method has <code class="docutils literal notranslate"><span class="pre">asyncio</span></code> equivalents</p></li>
<li><p>utilities for retries, fallbacks, schemas, and runtime configurability are available</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s1">&#39;Hi there!&#39;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">batch</span><span class="p">([</span><span class="s1">&#39;Hi there!&#39;</span><span class="p">,</span> <span class="s1">&#39;Bye!&#39;</span><span class="p">])</span>

<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="s1">&#39;Bye!&#39;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="llm-output">
<h3>LLM Output<a class="headerlink" href="#llm-output" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><p>LLM can generate specific output format, such as JSON, XML, CSV</p></li>
<li><dl>
<dt><strong>JSON Output</strong></dt><dd><ul class="simple">
<li><p>need to define schema using Pydantic, and include it in the prompt</p></li>
<li><p>schema is converted to <code class="docutils literal notranslate"><span class="pre">JSONSchema</span></code> object, and used to validate the output from LLM</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AnswerWithJustification</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;An answer to the user&#39;s question along with justification for the answer. &#39;&#39;&#39;</span>
    <span class="n">answer</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">justification</span><span class="p">:</span> <span class="nb">str</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;supported_model&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">structured_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">AnswerWithJustification</span><span class="p">)</span>
<span class="n">msg</span> <span class="o">=</span> <span class="n">structured_llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;Question&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Output Parsers</strong></dt><dd><ul class="simple">
<li><p>classes to structure LLM responses</p></li>
<li><p>can be used to provide output format instructions in the prompt</p></li>
<li><p>textual output can be rendered to a more structured format</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">CommaSeparatedListOutputParser</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">CommaSeparatedListOutputParser</span><span class="p">()</span>
<span class="n">items</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;apple, banana, cherry&quot;</span><span class="p">)</span> <span class="c1"># [&#39;apple&#39;, &#39;banana&#39;, &#39;cherry&#39;]</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="component-composition">
<h3>Component Composition<a class="headerlink" href="#component-composition" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><dl>
<dt><strong>Imperative Composition</strong></dt><dd><ul class="simple">
<li><p>calling components directly, e.g. <code class="docutils literal notranslate"><span class="pre">model.invoke()</span></code></p></li>
<li><p>Parallel execution: threads or coroutines in Python, and <code class="docutils literal notranslate"><span class="pre">Promise.all</span></code> in JavaScript</p></li>
<li><p>Streaming: using <code class="docutils literal notranslate"><span class="pre">yield</span></code></p></li>
<li><p>Async execution: with async functions</p></li>
<li><p>useful for writing custom logic</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">template</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;You are a helpful assistant.&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;human&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{question}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span>

<span class="c1"># combine components in a function</span>
<span class="nd">@chain</span>
<span class="k">def</span> <span class="nf">chatbot</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">template</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">token</span>

<span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">chatbot</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;Question&quot;</span><span class="p">}):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">part</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Declarative Composition</strong></dt><dd><ul class="simple">
<li><p>using LCEL (LangChain Expression Language)</p></li>
<li><p>LCEL compositions are compiled to an optimised execution plan</p></li>
<li><p>Streaming, Parallel and Async executions are automatic</p></li>
<li><p>useful for assembling existing components with limited customisation</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">template</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;You are a helpful assistant.&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;human&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{question}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span>

<span class="c1"># combine components with | operator</span>
<span class="n">chatbot</span> <span class="o">=</span> <span class="n">template</span> <span class="o">|</span> <span class="n">model</span>

<span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">chatbot</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;Question&quot;</span><span class="p">}):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">part</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p><a class="reference external" href="#langchain">back to top</a></p>
</section>
</section>
<section id="rag">
<h2>RAG<a class="headerlink" href="#rag" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#data-indexing">Data Indexing</a>, <a class="reference internal" href="#indexing-optimisations">Indexing Optimisations</a>, <a class="reference internal" href="#query-transformation">Query Transformation</a>, <a class="reference internal" href="#query-routing">Query Routing</a></p></li>
<li><p><a class="reference internal" href="#query-construction">Query Construction</a></p></li>
</ul>
<section id="data-indexing">
<h3>Data Indexing<a class="headerlink" href="#data-indexing" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><p>indexing is a technique to enhance LLM output by providing context from external sources</p></li>
<li><p>processing external data source, and storing embeddings in a vector store</p></li>
<li><p>embed a user’s query, retrieve similar documents, and passing them as context to the
prompt</p></li>
<li><p>Retrieving: getting relevant embeddings and data stored in the vector store based on
user’s query</p></li>
<li><p>Generation: synthesising original prompt with the retrieved relevant documents</p></li>
<li><p>Ingestion: converting documents into embeddings, and storing in vector store</p></li>
<li><p>Context Window: size of input and output tokens LLMs and embedding models can handle</p></li>
<li><dl class="simple">
<dt><strong>Document Loader</strong></dt><dd><ul class="simple">
<li><p>can load files such as txt, csv, json, Markdown, and integrate with platforms such
as Slack and Notion</p></li>
<li><p>can use <code class="docutils literal notranslate"><span class="pre">WebBaseLoader</span></code> to load HTML, or <code class="docutils literal notranslate"><span class="pre">PyPDFLoader</span></code> with <code class="docutils literal notranslate"><span class="pre">pypdf</span></code> package</p></li>
<li><p>loaded data is stored in <code class="docutils literal notranslate"><span class="pre">Document</span></code> class, and need to be split into chunks
semantically</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt><code class="docutils literal notranslate"><span class="pre">RecursiveCharacterTextSplitter</span></code></dt><dd><ul class="simple">
<li><p>can split text based on a list of separators in order</p></li>
<li><p>default separator order: <code class="docutils literal notranslate"><span class="pre">\n\n</span></code> (paragraph), <code class="docutils literal notranslate"><span class="pre">\n</span></code> (line), space (word)</p></li>
<li><p>split paragraphs that are within the chunk size</p></li>
<li><p>for paragraphs longer than the chunk size, split by the next separator</p></li>
<li><p>each chunk is a <code class="docutils literal notranslate"><span class="pre">Document</span></code> with metadata of the original document</p></li>
<li><p>can use for others, such as code languages and Markdown, with relevant separators</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_text_splitters</span> <span class="kn">import</span> <span class="n">Language</span><span class="p">,</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">TextLoader</span>

<span class="n">loader</span> <span class="o">=</span> <span class="n">TextLoader</span><span class="p">(</span><span class="s2">&quot;./main.py&quot;</span><span class="p">)</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="n">splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="o">.</span><span class="n">from_language</span><span class="p">(</span>
    <span class="n">language</span><span class="o">=</span><span class="n">Language</span><span class="o">.</span><span class="n">PYTHON</span><span class="p">,</span>
    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>

<span class="n">python_docs</span> <span class="o">=</span> <span class="n">splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Embedding</strong></dt><dd><ul class="simple">
<li><p>converting text to numbers that cannot be used to recover original text</p></li>
<li><p>both text and numerals are stored since it is a lossy representation</p></li>
<li><p>Dense embeddings: low-dimensional vectors with mostly non-zero values</p></li>
<li><p>Sparse embeddings: high-dimensional vectors with mostly zero values</p></li>
<li><p>never combine embeddings from different models</p></li>
<li><p>words or sentences that are close in meaning should be closer in semantic dimension</p></li>
<li><p>cosine similarity is usually used for degree of similarity</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Embeddings</span></code> class interfaces with text embedding models, and generate vector
representations</p></li>
<li><p>can embed documents and query</p></li>
<li><p>embedding multiple documents at the same time is more efficient</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">([</span>
    <span class="s2">&quot;Hi there!&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Oh, hello!&quot;</span><span class="p">,</span>
    <span class="s2">&quot;What&#39;s your name?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;My friends call me World&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Hello World!&quot;</span>
<span class="p">])</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Vector Store</strong></dt><dd><ul class="simple">
<li><p>database to store vectors and perform complex calculations</p></li>
<li><p>handle unstructured data, including text and images</p></li>
<li><p>has capabilities such as multi-tenancy and metadata filtering</p></li>
<li><p>PostgreSQL can be used as vector store with <code class="docutils literal notranslate"><span class="pre">pgvector</span></code> extension</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">add_documents()</span></code>: create embeddings for each document, and store them</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">connection</span> <span class="o">=</span> <span class="s1">&#39;PostgreSQL_Connection&#39;</span>

<span class="n">db</span> <span class="o">=</span> <span class="n">PGVector</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings_model</span><span class="p">,</span> <span class="n">connection</span><span class="o">=</span><span class="n">connection</span><span class="p">)</span>

<span class="n">db</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="s2">&quot;query&quot;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>

<span class="n">db</span><span class="o">.</span><span class="n">add_documents</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">Document</span><span class="p">(</span>
            <span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Content&quot;</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;value&quot;</span><span class="p">}</span>
        <span class="p">)</span>
    <span class="p">],</span>
    <span class="n">ids</span><span class="o">=</span><span class="n">ids</span>
<span class="p">)</span>

<span class="n">db</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">])</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Indexing API</strong></dt><dd><ul class="simple">
<li><p>uses <code class="docutils literal notranslate"><span class="pre">RecordManager</span></code> to track document writes into the vector store</p></li>
<li><p>stores document hash, write time, and source ID</p></li>
<li><p>provides cleanup modes to delete existing documents</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code>: manual clean up of old content</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Icremental</span></code> &amp; <code class="docutils literal notranslate"><span class="pre">full</span></code>: delete previous versions if content of the source document or
derived ones change</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Full</span></code>: delete any documents not included in documents currently being indexed</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.indexes</span> <span class="kn">import</span> <span class="n">SQLRecordManager</span><span class="p">,</span> <span class="n">index</span>

<span class="n">collection_name</span> <span class="o">=</span> <span class="s2">&quot;my_docs&quot;</span>
<span class="n">embeddings_model</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">)</span>
<span class="n">namespace</span> <span class="o">=</span> <span class="s2">&quot;my_docs_namespace&quot;</span>

<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">PGVector</span><span class="p">(</span>
    <span class="n">embeddings</span><span class="o">=</span><span class="n">embeddings_model</span><span class="p">,</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span>
    <span class="n">connection</span><span class="o">=</span><span class="n">connection</span><span class="p">,</span>
    <span class="n">use_jsonb</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">record_manager</span> <span class="o">=</span> <span class="n">SQLRecordManager</span><span class="p">(</span>
    <span class="n">namespace</span><span class="p">,</span>
    <span class="n">db_url</span><span class="o">=</span><span class="n">connection</span>
<span class="p">)</span>

<span class="n">record_manager</span><span class="o">.</span><span class="n">create_schema</span><span class="p">()</span>

<span class="n">docs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">Document</span><span class="p">(</span>
        <span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;content 1&quot;</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;source_1.txt&quot;</span><span class="p">}</span>
    <span class="p">),</span>
    <span class="n">Document</span><span class="p">(</span>
        <span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;content 2&quot;</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;source_2.txt&quot;</span><span class="p">}</span>
    <span class="p">)</span>
<span class="p">]</span>

<span class="n">index_1</span> <span class="o">=</span> <span class="n">index</span><span class="p">(</span>
    <span class="n">docs</span><span class="p">,</span>
    <span class="n">record_manager</span><span class="p">,</span>
    <span class="n">vectorstore</span><span class="p">,</span>
    <span class="n">cleanup</span><span class="o">=</span><span class="s2">&quot;incremental&quot;</span><span class="p">,</span>
    <span class="n">source_id_key</span><span class="o">=</span><span class="s2">&quot;source&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Index attempt 1: &quot;</span><span class="p">,</span> <span class="n">index_1</span><span class="p">)</span>

<span class="n">index_2</span> <span class="o">=</span> <span class="n">index</span><span class="p">(</span>
    <span class="n">docs</span><span class="p">,</span>
    <span class="n">record_manager</span><span class="p">,</span>
    <span class="n">vectorstore</span><span class="p">,</span>
    <span class="n">cleanup</span><span class="o">=</span><span class="s2">&quot;incremental&quot;</span><span class="p">,</span>
    <span class="n">source_id_key</span><span class="o">=</span><span class="s2">&quot;source&quot;</span>
<span class="p">)</span>

<span class="c1"># attempting to index again will not add the documents</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Index attempt 2: &quot;</span><span class="p">,</span> <span class="n">index_2</span><span class="p">)</span>

<span class="n">docs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span> <span class="o">=</span> <span class="s2">&quot;modified&quot;</span>

<span class="n">index_3</span> <span class="o">=</span> <span class="n">index</span><span class="p">(</span>
    <span class="n">docs</span><span class="p">,</span>
    <span class="n">record_manager</span><span class="p">,</span>
    <span class="n">vectorstore</span><span class="p">,</span>
    <span class="n">cleanup</span><span class="o">=</span><span class="s2">&quot;incremental&quot;</span><span class="p">,</span>
    <span class="n">source_id_key</span><span class="o">=</span><span class="s2">&quot;source&quot;</span>
<span class="p">)</span>

<span class="c1"># new version is written, and all old versions sharing the same source are deleted</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Index attempt 3: &quot;</span><span class="p">,</span> <span class="n">index_3</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="indexing-optimisations">
<h3>Indexing Optimisations<a class="headerlink" href="#indexing-optimisations" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><dl>
<dt><code class="docutils literal notranslate"><span class="pre">MultiVectorRetriever</span></code></dt><dd><ul class="simple">
<li><p>decouple documents to use for answer synthesis</p></li>
<li><p>e.g. in a document of text and tables, embed summaries of table elements with an id
reference to the full raw table, which is stored in a separate Docstore</p></li>
<li><p>enables to provide the model with full context to answer user’s question</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain.storage</span> <span class="kn">import</span> <span class="n">InMemoryStore</span>
<span class="kn">from</span> <span class="nn">langchain_postgres</span> <span class="kn">import</span> <span class="n">PGVector</span>
<span class="kn">from</span> <span class="nn">langchain.retrievers.multi_vector</span> <span class="kn">import</span> <span class="n">MultiVectorRetriever</span>

<span class="c1"># load the document, split, create embeddings and LLM model</span>

<span class="n">prompt_text</span> <span class="o">=</span> <span class="s2">&quot;Summarize the following document:</span><span class="se">\n\n</span><span class="si">{doc}</span><span class="s2">&quot;</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">prompt_text</span><span class="p">)</span>

<span class="n">summarise_chain</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;doc&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">page_content</span><span class="p">}</span> <span class="o">|</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>

<span class="n">summaries</span> <span class="o">=</span> <span class="n">summarise_chain</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">chunks</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;max_concurrency&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">})</span>

<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">PGVector</span><span class="p">(</span>
    <span class="n">embeddings</span><span class="o">=</span><span class="n">embeddings_model</span><span class="p">,</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span>
    <span class="n">connection</span><span class="o">=</span><span class="n">connection</span><span class="p">,</span>
    <span class="n">use_jsonb</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="c1"># for parent documents</span>
<span class="n">store</span> <span class="o">=</span> <span class="n">InMemoryStore</span><span class="p">()</span>
<span class="n">id_key</span> <span class="o">=</span> <span class="s2">&quot;doc_id&quot;</span>

<span class="n">retriever</span> <span class="o">=</span> <span class="n">MultiVectorRetriever</span><span class="p">(</span>
    <span class="n">vectorstore</span><span class="o">=</span><span class="n">vectorstore</span><span class="p">,</span>
    <span class="n">docstore</span><span class="o">=</span><span class="n">store</span><span class="p">,</span>
    <span class="n">id_key</span><span class="o">=</span><span class="n">id_key</span>
<span class="p">)</span>

<span class="n">doc_ids</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">]</span>

<span class="n">summary_docs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">id_key</span><span class="p">:</span> <span class="n">doc_ids</span><span class="p">[</span><span class="n">i</span><span class="p">]})</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">summaries</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">retriever</span><span class="o">.</span><span class="n">vectorstore</span><span class="o">.</span><span class="n">add_documents</span><span class="p">(</span><span class="n">summary_docs</span><span class="p">)</span>

<span class="c1"># store the original documents, linked to summaries via doc_ids</span>
<span class="n">retriever</span><span class="o">.</span><span class="n">docstore</span><span class="o">.</span><span class="n">mset</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">doc_ids</span><span class="p">,</span> <span class="n">chunks</span><span class="p">)))</span>

<span class="c1"># vector store retrieves the summaries</span>
<span class="n">sub_docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="o">.</span><span class="n">vectorstore</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span>
    <span class="s2">&quot;topic&quot;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># retriever return the larger source document chunks</span>
<span class="n">retrieved_docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;topic&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>RAPTOR</strong></dt><dd><ul class="simple">
<li><p>Recursive Abstractive Processing for Tree-Organised Retrieval</p></li>
<li><p>creating document summaries for higher-level concepts, embedding and clustering them
and summarising each cluster</p></li>
<li><p>recursively done to produce a tree of higher-level summaries</p></li>
<li><p>then the summaries and initial documents are indexed together</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>ColBERT</strong></dt><dd><ul class="simple">
<li><p>effective embeddings approach for better retrieval</p></li>
<li><p>generate contextual embeddings for each token in the document and query</p></li>
<li><p>calculate and score similarity between each query token and all document tokens</p></li>
<li><p>sum the max similarity score of each query embedding to any of the document
embeddings to get a score for each document</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="query-transformation">
<h3>Query Transformation<a class="headerlink" href="#query-transformation" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><p>modifying user input to be more or less abstract to generate accurate LLM output</p></li>
<li><dl>
<dt><strong>Rewrite-Retrieve-Read</strong></dt><dd><ul class="simple">
<li><p>prompts the LLM to rewrite the user’s query before performing retrieval</p></li>
<li><p>remove irrelevant information in the query with the help of LLM</p></li>
<li><p>but will add additional latency in the chain due to more LLM calls</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rewrite_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Provide a better search query for web search engine to answer the given</span>
<span class="s2">question, end the queries with &#39;**&#39;. Question: </span><span class="si">{x}</span><span class="s2"> Answer:</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">parse_rewriter_output</span><span class="p">(</span><span class="n">message</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;&quot;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s2">&quot;**&quot;</span><span class="p">)</span>

<span class="n">rewriter</span> <span class="o">=</span> <span class="n">rewrite_prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parse_rewriter_output</span>

<span class="nd">@chain</span>
<span class="k">def</span> <span class="nf">qa_rrr</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="n">new_query</span> <span class="o">=</span> <span class="n">rewriter</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">new_query</span><span class="p">)</span>
    <span class="n">formatted</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="nb">input</span><span class="p">})</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">formatted</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">answer</span>

<span class="n">qa_rrr</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Query with irrelevant information&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Multi-Query Retrieval</strong></dt><dd><ul class="simple">
<li><p>tell LLM to generate multiple queries based on the user’s initial one</p></li>
<li><p>each query is retrieved in parallel and inserted as prompt context for final output</p></li>
<li><p>useful when a single question may rely on multiple perspectives for an answer</p></li>
<li><p>should deduplicate documents as single retriever is used with multiple queries</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">perspectives_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are an AI language model assistant. Your task is to generate five</span>
<span class="s2">different versions of the given user question to retrieve relvant documents</span>
<span class="s2">from a vector database. By generating multiple perspectives on the user</span>
<span class="s2">question, your goal is to help the user overcome come of the limitations of</span>
<span class="s2">the distance-based similarity search. Provide these alternative questions</span>
<span class="s2">separated by newlines. Original question: </span><span class="si">{question}</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">parse_queries_output</span><span class="p">(</span><span class="n">message</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">query_gen</span> <span class="o">=</span> <span class="n">perspectives_prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parse_queries_output</span>

<span class="k">def</span> <span class="nf">get_unique_union</span><span class="p">(</span><span class="n">document_lists</span><span class="p">):</span>
    <span class="n">deduped_docs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">doc</span><span class="o">.</span><span class="n">page_content</span><span class="p">:</span> <span class="n">doc</span>
        <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">document_lists</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">sublist</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">deduped_docs</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

<span class="n">retrieval_chain</span> <span class="o">=</span> <span class="n">query_gen</span> <span class="o">|</span> <span class="n">retriever</span><span class="o">.</span><span class="n">batch</span> <span class="o">|</span> <span class="n">get_unique_union</span>

<span class="nd">@chain</span>
<span class="k">def</span> <span class="nf">multi_query_qa</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="n">retrieval_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">formatted</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="nb">input</span><span class="p">})</span>
    <span class="n">ans</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">formatted</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ans</span>

<span class="n">multi_query_qa</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Question&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>RAG-Fusion</strong></dt><dd><ul class="simple">
<li><p>similar to the Multi-Query retrieval</p></li>
<li><p>retrieved documents are re-ranked at the final step with RRF (Reciprocal Rank
Fusion) algorithm, pulling the most relevant documents to the top</p></li>
<li><p>RRF is ideal for combining results from queries with different scales or
distributions of scores</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># def multi_query_qa()</span>

<span class="n">prompt_rag_fusion</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are a helpful assistant that generates multiple search queries based on</span>
<span class="s2">a single input query.</span><span class="se">\n</span>
<span class="s2">Generate multiple search queries related to: </span><span class="si">{question}</span><span class="s2"> </span><span class="se">\n</span>
<span class="s2">Output (4 queries):</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">query_gen</span> <span class="o">=</span> <span class="n">prompt_rag_fusion</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parse_queries_output</span>

<span class="n">retrieval_chain</span> <span class="o">=</span> <span class="n">query_gen</span> <span class="o">|</span> <span class="n">retriever</span><span class="o">.</span><span class="n">batch</span> <span class="o">|</span> <span class="n">reciprocal_rank_fusion</span>

<span class="n">multi_query_qa</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Question&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">reciprocal_rank_fusion</span><span class="p">(</span><span class="n">results</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="mi">60</span><span class="p">):</span>
    <span class="n">fused_scores</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">docs</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
            <span class="n">doc_str</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">page_content</span>
            <span class="k">if</span> <span class="n">doc_str</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">fused_scores</span><span class="p">:</span>
                <span class="n">fused_scores</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">documents</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span> <span class="o">=</span> <span class="n">doc</span>

            <span class="n">fused_scores</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="n">k</span><span class="p">)</span>

    <span class="n">reranked_doc_strs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
        <span class="n">fused_scores</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">fused_scores</span><span class="p">[</span><span class="n">d</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">documents</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span> <span class="k">for</span> <span class="n">doc_str</span> <span class="ow">in</span> <span class="n">reranked_doc_strs</span><span class="p">]</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>HyDE</strong></dt><dd><ul class="simple">
<li><p>Hypothetical Document Embeddings</p></li>
<li><p>create hypothetical document based on user’s query, embed it, and retrieve relevant
documents based on vector similarity</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prompt_hyde</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Please write a passage to answer the question.</span><span class="se">\n</span>
<span class="s2">Question: </span><span class="si">{question}</span><span class="s2"> </span><span class="se">\n</span>
<span class="s2">Passage:</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Answer the following question based on this context:</span>

<span class="si">{context}</span>

<span class="s2">Question: </span><span class="si">{question}</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">generate_doc</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>

<span class="n">retrieval_chain</span> <span class="o">=</span> <span class="n">generate_doc</span> <span class="o">|</span> <span class="n">retriever</span>

<span class="nd">@chain</span>
<span class="k">def</span> <span class="nf">qa</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="n">retrieval_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">formatted</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="nb">input</span><span class="p">})</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">formatted</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">answer</span>

<span class="n">qa</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Question&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="query-routing">
<h3>Query Routing<a class="headerlink" href="#query-routing" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><p>to forward user’s query to the relevant data source</p></li>
<li><dl>
<dt><strong>Logical Routing</strong></dt><dd><ul class="simple">
<li><p>let LLM decide which data source to apply based on the query</p></li>
<li><p>function-calling models are used to help classify each query</p></li>
<li><p>need to define a schema that the model can use to generate arguments of a function
based on the query</p></li>
<li><p>extracted data source can be passed into other functions for additional logic</p></li>
<li><p>suitable when a defined list of data sources is available</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Literal</span>

<span class="k">class</span> <span class="nc">RouteQuery</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">datasource</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;source_1&quot;</span><span class="p">,</span> <span class="s2">&quot;source_2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;Given a user question, choose which datasource would be</span>
<span class="s2">        most relevant for answering their question</span>
<span class="s2">        &quot;&quot;&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">choose_route</span><span class="p">(</span><span class="n">result</span><span class="p">):</span>
    <span class="k">if</span> <span class="s2">&quot;source_1&quot;</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">datasource</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
        <span class="k">return</span> <span class="s2">&quot;chain for source_1&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;chain for source_2&quot;</span>

<span class="n">structured_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">RouteQuery</span><span class="p">)</span>

<span class="n">system</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;You are an expert at routing a user question to the appropriate</span>
<span class="s2">data source.</span>

<span class="s2">Based on the programming language the question is referring to, route it to</span>
<span class="s2">the relevant data source.</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="n">system</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{question}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">router</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">structured_llm</span>

<span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;Question&quot;</span>

<span class="c1"># chaining for additional logic</span>
<span class="n">full_chain</span> <span class="o">=</span> <span class="n">router</span> <span class="o">|</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">choose_route</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">full_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Semantic Routing</strong></dt><dd><ul class="simple">
<li><p>embedding various prompts of various data sources with the query, and doing vector
similarity search for the most similar prompt</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain.utils.math</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>

<span class="n">template_1</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Template 1</span>
<span class="s2">Here is a question:</span>
<span class="si">{query}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">template_2</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Template 2</span>
<span class="s2">Here is a question:</span>
<span class="si">{query}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">prompt_templates</span> <span class="o">=</span> <span class="p">[</span><span class="n">template_1</span><span class="p">,</span> <span class="n">template_2</span><span class="p">]</span>
<span class="n">prompt_embeddings</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">(</span><span class="n">prompt_templates</span><span class="p">)</span>

<span class="nd">@chain</span>
<span class="k">def</span> <span class="nf">prompt_router</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
    <span class="n">query_embedding</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">([</span><span class="n">query_embedding</span><span class="p">],</span> <span class="n">prompt_embeddings</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">most_similar</span> <span class="o">=</span> <span class="n">prompt_templates</span><span class="p">[</span><span class="n">similarity</span><span class="o">.</span><span class="n">argmax</span><span class="p">()]</span>
    <span class="k">return</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">most_similar</span><span class="p">)</span>

<span class="n">semantic_router</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">prompt_router</span>
    <span class="o">|</span> <span class="n">llm</span>
    <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">semantic_router</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Question&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="query-construction">
<h3>Query Construction<a class="headerlink" href="#query-construction" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><p>convert natural language query into language of database or data source</p></li>
<li><dl>
<dt><strong>Text-to-Metadata Filter</strong></dt><dd><ul class="simple">
<li><p>can attach metadata key-value pairs to vectors in an index during embedding process</p></li>
<li><p>filter expressions will be used during query</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SelfQueryRetriever</span></code> uses LLM to extract and execute relevant metadata filters based
on user’s query and predefined metadata schema</p></li>
<li><p>retriever will send query generation prompt, parse metadata filter and rewritten
query, convert the metadata filter for vector store, and run similarity search
against the vector store</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.chains.query_constructor.schema</span> <span class="kn">import</span> <span class="n">AttributeInfo</span>
<span class="kn">from</span> <span class="nn">langchain.retrievers.self_query.base</span> <span class="kn">import</span> <span class="n">SelfQueryRetriever</span>

<span class="n">fields</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">AttributeInfo</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;NAME&quot;</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;DESC&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string or list[string]&quot;</span>
    <span class="p">),</span>
<span class="p">]</span>

<span class="n">description</span> <span class="o">=</span> <span class="s2">&quot;DESC&quot;</span>

<span class="n">retriever</span> <span class="o">=</span> <span class="n">SelfQueryRetriever</span><span class="o">.</span><span class="n">from_llm</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">db</span><span class="p">,</span> <span class="n">description</span><span class="p">,</span> <span class="n">fields</span><span class="p">)</span>

<span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Question&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Text-to-SQL</strong></dt><dd><ul class="simple">
<li><p>Database description: provide LLM with accurate description of the database, such as
<code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">TABLE</span></code> description for each table with column names and types, and can also
include example rows from the table</p></li>
<li><p>Few-shot examples: append standard static examples in the prompt to guide the agent
on how it should build queries based on questions</p></li>
<li><p>always run queries with a user with read-only permissions</p></li>
<li><p>database user running the queries should have access only to the necessary tables</p></li>
<li><p>add a time-out to the queries to protect from expensive query</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_community.tools.sql_database.tool</span> <span class="kn">import</span> <span class="n">QuerySQLDatabaseTool</span>
<span class="kn">from</span> <span class="nn">langchain_community.utilities</span> <span class="kn">import</span> <span class="n">SQLDatabase</span>
<span class="kn">from</span> <span class="nn">langchain.chains.sql_database.query</span> <span class="kn">import</span> <span class="n">create_sql_query_chain</span>

<span class="n">db</span> <span class="o">=</span> <span class="n">SQLDatabase</span><span class="o">.</span><span class="n">from_uri</span><span class="p">(</span><span class="n">connection</span><span class="p">)</span>

<span class="n">write_query</span> <span class="o">=</span> <span class="n">create_sql_query_chain</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">db</span><span class="p">)</span>
<span class="n">execute_query</span> <span class="o">=</span> <span class="n">QuerySQLDatabaseTool</span><span class="p">(</span><span class="n">db</span><span class="o">=</span><span class="n">db</span><span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">write_query</span> <span class="o">|</span> <span class="n">execute_query</span>
<span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s1">&#39;Question&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p><a class="reference external" href="#langchain">back to top</a></p>
</section>
</section>
<section id="langgraph">
<h2>LangGraph<a class="headerlink" href="#langgraph" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#graph">Graph</a>, <a class="reference internal" href="#memory">Memory</a>, <a class="reference internal" href="#multiactor">Multiactor</a>, <a class="reference internal" href="#chat-history">Chat History</a>, <a class="reference internal" href="#subgraphs">Subgraphs</a></p></li>
</ul>
<section id="graph">
<h3>Graph<a class="headerlink" href="#graph" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>LangGraph is an open source library by LangChain</p></li>
<li><p>enable developers to implement multiactor, multistep, and stateful cognitive
architectures called graphs</p></li>
<li><p>State: data received from outside, modified and produced by the app</p></li>
<li><p>Node: Python or JavaScript functions, receiving current state and returning updated state</p></li>
<li><p>Edge: connection between nodes, can be fixed path or conditional</p></li>
<li><p>need to define the state of the graph first</p></li>
<li><p>state keys without an annotation will be overwritten</p></li>
<li><p>without explicit instruction, execution is stopped when there’s no more nodes to run</p></li>
<li><p>graph is compiled into a runnable object</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span><span class="p">,</span> <span class="n">TypedDict</span>
<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span>
<span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">END</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">StateGraph</span>
<span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>

<span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>

<span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">chatbot</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">answer</span><span class="p">]}</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot&quot;</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">)</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>

<span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>

<span class="nb">input</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s1">&#39;hi!&#39;</span><span class="p">)]}</span>

<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</section>
<section id="memory">
<h3>Memory<a class="headerlink" href="#memory" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>LLMs are stateless, with prior prompt or model response is lost with a new response</p></li>
<li><p>including previous conversations and context in the final prompt can give memory</p></li>
<li><p>chat history can be stored as a list of messages, append recent messages after each turn,
or append into prompt by inserting the messages into the prompt</p></li>
<li><p>appending chat history in the prompt have scalability issues</p></li>
<li><p>Checkpointer: storage adapter for in-memory, SQLite, Postgres, Redis, and MySQL</p></li>
<li><p>Thread: also called interaction, auto created when first used</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="kn">from</span> <span class="nn">langchain_core.runnables.config</span> <span class="kn">import</span> <span class="n">RunnableConfig</span>

<span class="c1"># stores the state at the end of each step</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">MemorySaver</span><span class="p">())</span>

<span class="n">thread_1</span> <span class="o">=</span> <span class="n">RunnableConfig</span><span class="p">({</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}})</span>
<span class="n">result_1</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;hi, my name is Jack!&quot;</span><span class="p">)]},</span> <span class="n">thread_1</span>
<span class="p">)</span>

<span class="n">result_2</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;what is my name?&quot;</span><span class="p">)]},</span> <span class="n">thread_1</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</section>
<section id="multiactor">
<h3>Multiactor<a class="headerlink" href="#multiactor" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>application with multiple actors needs a coordination layer to define actors, hand off
work, and schedule execution of each actor</p></li>
<li><p>each actor should help update a single central state</p></li>
<li><p>with a single  central state, a snapshot can be made, execution can be paused and
human-in-the-loop control can be implemented</p></li>
</ul>
</div></blockquote>
</section>
<section id="chat-history">
<h3>Chat History<a class="headerlink" href="#chat-history" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><p>chat history messages should be in a format to generate accurate response from the model</p></li>
<li><dl>
<dt><strong>Trimming Messages</strong></dt><dd><ul class="simple">
<li><p>limit the number of messages that are retrieved from history and appended to the
prompt</p></li>
<li><p>ideal to load and store the most recent messages</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trim_messages()</span></code>: can specify how many tokens to keep or remove from chat history,
and has different strategies</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="p">(</span><span class="n">AIMessage</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">SystemMessage</span><span class="p">,</span>
                                     <span class="n">trim_messages</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="n">trimmer</span> <span class="o">=</span> <span class="n">trim_messages</span><span class="p">(</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">65</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;last&quot;</span><span class="p">,</span> <span class="c1"># prioritise most recent</span>
    <span class="n">token_counter</span><span class="o">=</span><span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o&quot;</span><span class="p">),</span> <span class="c1"># use tokeniser appropriate to that model</span>
    <span class="n">include_system</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># keep system message</span>
    <span class="n">allow_partial</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># to cut the last message&#39;s content to fit or not</span>
    <span class="n">start_on</span><span class="o">=</span><span class="s2">&quot;human&quot;</span> <span class="c1"># never remove AIMessage without removing corresponding HumanMessage</span>
<span class="p">)</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;you&#39;re a good assistant&quot;</span><span class="p">),</span>
    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;hi! i&#39;m bob&quot;</span><span class="p">),</span>
    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;hi&quot;</span><span class="p">),</span>
    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;i like vanilla ice cream&quot;</span><span class="p">),</span>
    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;nice&quot;</span><span class="p">),</span>
    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;what&#39;s 2 + 2?&quot;</span><span class="p">),</span>
    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;4&quot;</span><span class="p">),</span>
    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;thanks&quot;</span><span class="p">),</span>
    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;no problem!&quot;</span><span class="p">),</span>
    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;having fun?&quot;</span><span class="p">),</span>
    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;yes&quot;</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">trimmer</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Filtering Messages</strong></dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">filter_messages()</span></code>: filter by type, ID, or name</p></li>
<li><p>can also be composed with other components in a chain</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">filter_messages</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;you&#39;re a good assistant&quot;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;1&quot;</span><span class="p">),</span>
    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;hi! i&#39;m bob&quot;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;2&quot;</span><span class="p">),</span>
    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;hi&quot;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;3&quot;</span><span class="p">),</span>
    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;i like vanilla ice cream&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;bob&quot;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;4&quot;</span><span class="p">),</span>
    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;nice&quot;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;5&quot;</span><span class="p">),</span>
    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;what&#39;s 2 + 2?&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;alice&quot;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;6&quot;</span><span class="p">),</span>
    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;4&quot;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;7&quot;</span><span class="p">),</span>
    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;thanks&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;alice&quot;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;8&quot;</span><span class="p">),</span>
    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;no problem!&quot;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;9&quot;</span><span class="p">),</span>
    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;having fun?&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;bob&quot;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;10&quot;</span><span class="p">),</span>
    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;yes&quot;</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;11&quot;</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">filter_messages</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">include_types</span><span class="o">=</span><span class="s2">&quot;human&quot;</span><span class="p">)</span>

<span class="n">filter_</span> <span class="o">=</span> <span class="n">filter_messages</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">include_types</span><span class="o">=</span><span class="p">[</span>
                <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">],</span> <span class="n">exclude_ids</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;3&quot;</span><span class="p">])</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">filter_</span> <span class="o">|</span> <span class="n">model</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Merging Consecutive Messages</strong></dt><dd><ul class="simple">
<li><p>models such as Anthropic chat models do not support consecutive messages of the same
type</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">merge_message_runs()</span></code>: allows to merge consecutive messages of the same type</p></li>
<li><p>a list will be merged as a list</p></li>
<li><p>can also be composed with other components in a chain</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">merge_message_runs</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;you&#39;re a good assistant&quot;</span><span class="p">),</span>
    <span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;you always respond with a joke&quot;</span><span class="p">),</span>
    <span class="n">HumanMessage</span><span class="p">(</span>
        <span class="p">[{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;hello&quot;</span><span class="p">}]</span>
    <span class="p">),</span>
    <span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;world&quot;</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">merger_</span> <span class="o">=</span> <span class="n">merge_message_runs</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>

<span class="c1"># SystemMessage(content=&quot;you&#39;re a good assistant\nyou always respond with a joke&quot;),</span>
<span class="c1"># HumanMessage(content=[{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;hello&quot;}, &quot;world&quot;]</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">merger_</span> <span class="o">|</span> <span class="n">model</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="subgraphs">
<h3>Subgraphs<a class="headerlink" href="#subgraphs" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><p>graphs that are used as part of another graph</p></li>
<li><p>to build multi-agent systems, reuse a set of nodes in multiple graphs, and let different
teams to work on different parts of the graph</p></li>
<li><dl>
<dt><strong>Direct Subgraph Call</strong></dt><dd><ul class="simple">
<li><p>adding a node that calls the subgraph directly to the parent</p></li>
<li><p>both should share state keys to communicate, and do not need to transform state</p></li>
<li><p>passing extra keys to the subgraph node will be ignored</p></li>
<li><p>extra keys from the subgraph will be ignored by the parent</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="n">foo</span><span class="p">:</span> <span class="nb">str</span>    <span class="c1"># shared with subgraph</span>

<span class="k">class</span> <span class="nc">SubgraphState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="n">foo</span><span class="p">:</span> <span class="nb">str</span>    <span class="c1"># shared with parent</span>
    <span class="n">bar</span><span class="p">:</span> <span class="nb">str</span>

<span class="k">def</span> <span class="nf">subgraph_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">SubgraphState</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;foo&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;bar&quot;</span><span class="p">}</span>

<span class="n">subgraph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">SubgraphState</span><span class="p">)</span>
<span class="n">subgraph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">subgraph_node</span><span class="p">)</span>
<span class="n">subgraph</span> <span class="o">=</span> <span class="n">subgraph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>

<span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;subgraph&quot;</span><span class="p">,</span> <span class="n">subgraph</span><span class="p">)</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Function Subgraph Call</strong></dt><dd><ul class="simple">
<li><p>adding a node with a function that invokes the subgraph to the parent</p></li>
<li><p>both with different state schemas</p></li>
<li><p>function needs to transform parent state to the subgraph state before invoking the
subgraph and transform the result back to the parent state before returning</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="n">foo</span><span class="p">:</span> <span class="nb">str</span>

<span class="k">class</span> <span class="nc">SubgraphState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="n">bar</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">baz</span><span class="p">:</span> <span class="nb">str</span>

<span class="k">def</span> <span class="nf">subgraph_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">SubgraphState</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;bar&quot;</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;bar&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;baz&quot;</span><span class="p">}</span>

<span class="k">def</span> <span class="nf">node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">subgraph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;bar&quot;</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;foo&quot;</span><span class="p">]})</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;foo&quot;</span><span class="p">:</span> <span class="n">response</span><span class="p">[</span><span class="s2">&quot;bar&quot;</span><span class="p">]}</span>

<span class="n">subgraph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">SubgraphState</span><span class="p">)</span>
<span class="n">subgraph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">subgraph_node</span><span class="p">)</span>
<span class="n">subgraph</span> <span class="o">=</span> <span class="n">subgraph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>

<span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
</section>
<section id="langgraph-platform">
<h2>LangGraph Platform<a class="headerlink" href="#langgraph-platform" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#data-models">Data Models</a>, <a class="reference internal" href="#features">Features</a></p></li>
<li><p>managed service to deploy and host LangGraph agents</p></li>
<li><p>horizontally scales task queues, servers, and a Postgres checkpointer for efficiency</p></li>
<li><p>allows collaboration of deploying and monitoring agentic AI apps</p></li>
<li><p>LangGraph Studio: to debug, edit and test agents visually, can share agent with team members</p></li>
</ul>
<section id="data-models">
<h3>Data Models<a class="headerlink" href="#data-models" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt><strong>Assistants</strong></dt><dd><ul>
<li><p>configured instance of <code class="docutils literal notranslate"><span class="pre">CompiledGraph</span></code></p></li>
<li><p>has instance-specific configuration and metadata</p></li>
<li><p>multiple assistants can reference the same graph, but have different configuration
and metadata</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Threads</strong></dt><dd><ul>
<li><p>contains state collection of a group of runs</p></li>
<li><p>checkpoint: state of a thread at particular time</p></li>
<li><p>state of the underlying graph of the assistant will be persisted to the thread</p></li>
<li><p>current and historical state can be retrieved</p></li>
<li><p>a thread needs to be created before executing a run to persist state</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Runs</strong></dt><dd><ul>
<li><p>invocation of an assistant</p></li>
<li><p>each run can have its own input, configuration and metadata</p></li>
<li><p>can be executed on a thread</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Cron Jobs</strong></dt><dd><ul>
<li><p>allow to run graphs on a schedule</p></li>
<li><p>user must specify schedule, assistant, and input</p></li>
<li><p>a new thread will be created and given the input to run</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="features">
<h3>Features<a class="headerlink" href="#features" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt><strong>Streaming</strong></dt><dd><ul>
<li><p>streaming mode determines what data is streamed back to the client</p></li>
<li><p>Values: stream full state of the graph after each super-step is executed</p></li>
<li><p>Messages: stream complete messages and tokens, mostly for chat apps, and can only
use this mode if graph contains a <code class="docutils literal notranslate"><span class="pre">messages</span></code> key</p></li>
<li><p>Updates: stream state updates of the graph after each node execution</p></li>
<li><p>Events: stream all events during graph execution, can be used to do token-by-token
streaming for LLMs</p></li>
<li><p>Debug: stream debug events during graph execution</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Human-in-the-loop</strong></dt><dd><ul>
<li><p>LangGraph Platform allows human intervention to prevent unwanted outcomes</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Double Texting</strong></dt><dd><ul>
<li><p>Reject: reject and does not allow double texting</p></li>
<li><p>Enqueue: complete the first run, and sends the new input as separate run</p></li>
<li><p>Interrupt: save and interrupt current execution, and continue to run with new input</p></li>
<li><p>Rollback: roll back all work and run with new input</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Stateless Runs</strong></dt><dd><ul>
<li><p>take the input, create a thread, runs the agent without checkpoints, and clean the
thread</p></li>
<li><p>stateless runs are retried while keeping memory intact</p></li>
<li><p>for background runs, entire run will be retried if the task worker dies halfway</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Webhooks</strong></dt><dd><ul>
<li><p>LangGraph Platform supports completion webhooks</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p><a class="reference external" href="#langchain">back to top</a></p>
</section>
</section>
<section id="cognitive-architectures">
<h2>Cognitive Architectures<a class="headerlink" href="#cognitive-architectures" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#degree-of-autonomy">Degree of Autonomy</a>, <a href="#id10"><span class="problematic" id="id11">`LLM Call Architectures`_</span></a>, <a class="reference internal" href="#chain-architecture">Chain Architecture</a>, <a class="reference internal" href="#router-architecture">Router Architecture</a></p></li>
<li><p>cognitive architectures can be called a recipe for the steps to be taken by an LLM app</p></li>
<li><p>Agency: capacity to act autonomously</p></li>
<li><p>Reliability: degree to which agency’s outputs can be trusted</p></li>
<li><p>Major Architectures: Code (does not use LLMs, same as regular software), LLM Call, Chain,
Router, State Machine, Autonomous</p></li>
</ul>
<section id="degree-of-autonomy">
<h3>Degree of Autonomy<a class="headerlink" href="#degree-of-autonomy" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>measure by evaluating how much of the app behaviour is determined by LLM</p></li>
<li><p>check if LLM has decided the output of a step, the next step to take, and what steps
are available to take</p></li>
</ul>
</div></blockquote>
</section>
<section id="llm-call-architecture">
<h3>LLM Call Architecture<a class="headerlink" href="#llm-call-architecture" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>one LLM call only, useful when a large app make use of LLM</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">chatbot</span><span class="p">(</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">llm</span><span class="p">))</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</section>
<section id="chain-architecture">
<h3>Chain Architecture<a class="headerlink" href="#chain-architecture" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>multiple LLM calls in a predefined sequence, also called flow engineering</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">,</span> <span class="n">input_schema</span><span class="o">=</span><span class="n">Input</span><span class="p">,</span> <span class="n">output_schema</span><span class="o">=</span><span class="n">Output</span><span class="p">)</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;generate_sql&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">generate_sql</span><span class="p">(</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">llm_low_temp</span><span class="p">,</span> <span class="n">generate_prompt</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;explain_sql&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">explain_sql</span><span class="p">(</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">llm_high_temp</span><span class="p">,</span> <span class="n">explain_prompt</span><span class="p">))</span>  <span class="c1"># type: ignore</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;generate_sql&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;generate_sql&quot;</span><span class="p">,</span> <span class="s2">&quot;explain_sql&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;explain_sql&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</section>
<section id="router-architecture">
<h3>Router Architecture<a class="headerlink" href="#router-architecture" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>using LLM to define the sequence of steps to take</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">router_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">State</span><span class="p">:</span>
    <span class="n">user_message</span> <span class="o">=</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;user_query&quot;</span><span class="p">])</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">prompt</span><span class="p">,</span> <span class="o">*</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">],</span> <span class="n">user_message</span><span class="p">]</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;domain&quot;</span><span class="p">:</span> <span class="n">res</span><span class="o">.</span><span class="n">content</span><span class="p">,</span>
        <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">user_message</span><span class="p">,</span> <span class="n">res</span><span class="p">]</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">pick_retriever</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;retrieve_medical_records&quot;</span><span class="p">,</span>
                                            <span class="s2">&quot;retrieve_insurance_faqs&quot;</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;domain&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;records&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;retrieve_medical_records&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;retrieve_insurance_faqs&quot;</span>

<span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">,</span> <span class="n">input_schema</span><span class="o">=</span><span class="n">Input</span><span class="p">,</span> <span class="n">output_schema</span><span class="o">=</span><span class="n">Output</span><span class="p">)</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;router&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">router_node</span><span class="p">(</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">llm_low_temp</span><span class="p">,</span> <span class="n">router_prompt</span><span class="p">))</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;retrieve_medical_records&quot;</span><span class="p">,</span>
                 <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">retrieve_medical_records</span><span class="p">(</span>
                     <span class="n">state</span><span class="p">,</span> <span class="n">medical_records_retriever</span><span class="p">))</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;retrieve_insurance_faqs&quot;</span><span class="p">,</span>
                 <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">retrieve_insurance_faqs</span><span class="p">(</span>
                     <span class="n">state</span><span class="p">,</span> <span class="n">insurance_faqs_retriever</span><span class="p">))</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;generate_answer&quot;</span><span class="p">,</span>
                 <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">generate_answer</span><span class="p">(</span>
                     <span class="n">state</span><span class="p">,</span> <span class="n">llm_high_temp</span><span class="p">,</span>
                     <span class="n">medical_records_prompt</span><span class="p">,</span> <span class="n">insurance_faqs_prompt</span><span class="p">))</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;router&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;router&quot;</span><span class="p">,</span> <span class="n">pick_retriever</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;retrieve_medical_records&quot;</span><span class="p">,</span> <span class="s2">&quot;generate_answer&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;retrieve_insurance_faqs&quot;</span><span class="p">,</span> <span class="s2">&quot;generate_answer&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;generate_answer&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</section>
</section>
<section id="agent-architecture">
<h2>Agent Architecture<a class="headerlink" href="#agent-architecture" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#standard-agent">Standard Agent</a>, <a class="reference internal" href="#always-tool-calling-first">Always Tool Calling First</a>, <a class="reference internal" href="#managing-multiple-tools">Managing Multiple Tools</a>, <a class="reference internal" href="#reflection">Reflection</a>, <a class="reference internal" href="#multi-agent">Multi-agent</a></p></li>
<li><p>Agent: something that acts</p></li>
<li><p>uses an LLM to pick from one or more possible courses of action, given context of current
or desired next state</p></li>
<li><p>implemented by combining Tool Calling and Chain-of-Thought prompting techniques</p></li>
<li><p>LLM-driven Loop: plan actions and execute, LLM will decide when to stop looping</p></li>
<li><p>use a conditional edge to implement a loop as it can end the graph</p></li>
</ul>
<section id="standard-agent">
<h3>Standard Agent<a class="headerlink" href="#standard-agent" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>LLM is always called first to decide a tool, adapting the behaviour to each user
query</p></li>
<li><p>but flexibility can also cause unpredictability</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">ast</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span><span class="p">,</span> <span class="n">TypedDict</span>
<span class="kn">from</span> <span class="nn">langchain_community.tools</span> <span class="kn">import</span> <span class="n">DuckDuckGoSearchRun</span>
<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span>
<span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">Runnable</span>
<span class="kn">from</span> <span class="nn">langchain_core.tools</span> <span class="kn">import</span> <span class="n">tool</span>
<span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">START</span><span class="p">,</span> <span class="n">StateGraph</span>
<span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span> <span class="nn">langgraph.prebuilt</span> <span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>

<span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>


<span class="nd">@tool</span>
<span class="k">def</span> <span class="nf">calculator</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A simple calculator tool, Input should be a mathematical expression.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ast</span><span class="o">.</span><span class="n">literal_eval</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">llm_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">llm</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">State</span><span class="p">:</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">res</span><span class="p">}</span>

<span class="n">search</span> <span class="o">=</span> <span class="n">DuckDuckGoSearchRun</span><span class="p">()</span>
<span class="n">tools</span> <span class="o">=</span> <span class="p">[</span><span class="n">search</span><span class="p">,</span> <span class="n">calculator</span><span class="p">]</span>

<span class="n">llm</span><span class="p">:</span> <span class="n">Runnable</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4.1-mini&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools</span><span class="p">)</span>

<span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;llm&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">llm_node</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">llm</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="p">))</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;llm&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;llm&quot;</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="s2">&quot;llm&quot;</span><span class="p">)</span>

<span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>

<span class="nb">input</span><span class="p">:</span> <span class="n">State</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;Question&quot;&quot;&quot;</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">}</span>

<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</section>
<section id="always-tool-calling-first">
<h3>Always Tool Calling First<a class="headerlink" href="#always-tool-calling-first" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>having a clear rule that certain tool should always be called first</p></li>
<li><p>can reduce overall latency, and prevent erroneous LLM decision</p></li>
<li><p>but it can also make worse if there is no clear rule</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># does not call LLM, only creates a tool for the search tool</span>
<span class="k">def</span> <span class="nf">first_llm</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">State</span><span class="p">:</span>
    <span class="n">query</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>
    <span class="n">search_tool_call</span> <span class="o">=</span> <span class="n">ToolCall</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;duckduckgo_search&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">{</span>
                                <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">},</span> <span class="nb">id</span><span class="o">=</span><span class="n">uuid4</span><span class="p">()</span><span class="o">.</span><span class="n">hex</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">tool_calls</span><span class="o">=</span><span class="p">[</span><span class="n">search_tool_call</span><span class="p">])</span>
    <span class="p">}</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;first_llm&quot;</span><span class="p">,</span>
                 <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">first_llm</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;llm&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">llm_node</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">llm</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="p">))</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;first_llm&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;first_llm&quot;</span><span class="p">,</span> <span class="s2">&quot;tools&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;llm&quot;</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="s2">&quot;llm&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</section>
<section id="managing-multiple-tools">
<h3>Managing Multiple Tools<a class="headerlink" href="#managing-multiple-tools" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>LLMs struggle to choose the right one when given many tools</p></li>
<li><p>can use a RAG step to preselect the most relevant tools for current query</p></li>
<li><p>giving LLM only a subset of tools can reduce cost, but RAG step adds latency</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">llm_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">llm</span><span class="p">,</span> <span class="n">tools</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">State</span><span class="p">:</span>
    <span class="n">selected_tools</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">tool</span> <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="n">tools</span> <span class="k">if</span> <span class="n">tool</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;selected_tools&quot;</span><span class="p">]]</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">selected_tools</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">res</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">select_tools</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">tools_retriever</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">State</span><span class="p">:</span>
    <span class="n">query</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>
    <span class="n">tool_docs</span> <span class="o">=</span> <span class="n">tools_retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;selected_tools&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">doc</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;name&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">tool_docs</span><span class="p">]</span>
    <span class="p">}</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>
<span class="n">llm</span><span class="p">:</span> <span class="n">Runnable</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4.1-mini&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tools_retriever</span> <span class="o">=</span> <span class="n">InMemoryVectorStore</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span>
    <span class="p">[</span><span class="n">Document</span><span class="p">(</span><span class="n">tool</span><span class="o">.</span><span class="n">description</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span>
              <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">tool</span><span class="o">.</span><span class="n">name</span><span class="p">})</span> <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="n">tools</span><span class="p">],</span>
    <span class="n">embeddings</span>
<span class="p">)</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">()</span>

<span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;select_tools&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">select_tools</span><span class="p">(</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">tools_retriever</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;llm&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">llm_node</span><span class="p">(</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">llm</span><span class="p">,</span> <span class="n">tools</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="p">))</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;select_tools&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;select_tools&quot;</span><span class="p">,</span> <span class="s2">&quot;llm&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;llm&quot;</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;tools&quot;</span><span class="p">,</span> <span class="s2">&quot;llm&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</section>
<section id="reflection">
<h3>Reflection<a class="headerlink" href="#reflection" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>also called self-critique</p></li>
<li><p>allowing LLM to analyse past output, including past reflections, and refine it</p></li>
<li><p>need to have a loop between a creator prompt and a reviser prompt</p></li>
<li><p>can be combined with other prompting techniques</p></li>
<li><p>always cost higher latency, but likely to increase the quality of final output</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">State</span><span class="p">:</span>
    <span class="n">ans</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">prompt</span><span class="p">]</span> <span class="o">+</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">ans</span><span class="p">]}</span>


<span class="k">def</span> <span class="nf">reflect</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">State</span><span class="p">:</span>
    <span class="c1"># invert the messages</span>
    <span class="n">cls_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">AIMessage</span><span class="p">:</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">:</span> <span class="n">AIMessage</span><span class="p">}</span>
    <span class="n">translated</span> <span class="o">=</span> <span class="p">[</span><span class="n">prompt</span><span class="p">,</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+</span> <span class="p">[</span>
        <span class="n">cls_map</span><span class="p">[</span><span class="n">msg</span><span class="o">.</span><span class="vm">__class__</span><span class="p">](</span><span class="n">content</span><span class="o">=</span><span class="n">msg</span><span class="o">.</span><span class="n">content</span><span class="p">)</span> <span class="c1"># calling a constructor</span>
        <span class="k">for</span> <span class="n">msg</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">:]</span>
    <span class="p">]</span>
    <span class="n">ans</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">translated</span><span class="p">)</span>
    <span class="c1"># treat output as human feedback for generator</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">ans</span><span class="o">.</span><span class="n">content</span><span class="p">)]}</span>


<span class="k">def</span> <span class="nf">should_continue</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">6</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">END</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;reflect&quot;</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;generate&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">generate</span><span class="p">(</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">llm</span><span class="p">,</span> <span class="n">generate_prompt</span><span class="p">))</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;reflect&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">state</span><span class="p">:</span> <span class="n">reflect</span><span class="p">(</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">llm</span><span class="p">,</span> <span class="n">reflection_prompt</span><span class="p">))</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;generate&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;generate&quot;</span><span class="p">,</span> <span class="n">should_continue</span><span class="p">,</span> <span class="p">{</span>
    <span class="s2">&quot;reflect&quot;</span><span class="p">:</span> <span class="s2">&quot;reflect&quot;</span> <span class="c1"># only explicit mapping shows on graph image</span>
<span class="p">})</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;reflect&quot;</span><span class="p">,</span> <span class="s2">&quot;generate&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</section>
<section id="multi-agent">
<h3>Multi-agent<a class="headerlink" href="#multi-agent" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>composed of multiple smaller, independent agents</p></li>
<li><p>prevents an agent with multiple tools to make poor decisions</p></li>
<li><p>agents can be a simple prompt, LLM call or complex as ReAct agent</p></li>
<li><dl class="simple">
<dt><strong>Network Strategy</strong></dt><dd><ul>
<li><p>agents can communicate, and any agent can decide which to call next</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Hierarchical Strategy</strong></dt><dd><ul>
<li><p>system with a supervisor of supervisors</p></li>
<li><p>for more complex control flows</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Custom Multi-Agent Workflow</strong></dt><dd><ul>
<li><p>each communicate with only a subset of agents</p></li>
<li><p>parts of the flow are deterministic</p></li>
<li><p>only selected agents can decide which others to call next</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Supervisor Strategy</strong></dt><dd><ul>
<li><p>each agents communicates with the supervisor agent</p></li>
<li><p>supervisor decides which agent to call next</p></li>
<li><p>supervisor agent can be an LLM call with tools</p></li>
<li><p>subagent can be its own graph with internal state and only outputs summary of its
work</p></li>
<li><p>can make each subagent to decide to return output directly to user or not</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p><a class="reference external" href="#langchain">back to top</a></p>
</section>
</section>
<section id="llm-patterns">
<h2>LLM Patterns<a class="headerlink" href="#llm-patterns" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#structured-output">Structured Output</a>, <a class="reference internal" href="#streaming-output">Streaming Output</a>, <a class="reference internal" href="#human-in-the-loop">Human in the Loop</a>, <a class="reference internal" href="#double-texting-modes">Double Texting Modes</a></p></li>
<li><p>Agent: high agency, lower reliability</p></li>
<li><p>Chain: low agency, higher reliability</p></li>
<li><p>LLM apps should minimise latency (time to get final answer), autonomy (interruptions for
human input), or variance (variation between invocations)</p></li>
</ul>
<section id="structured-output">
<h3>Structured Output<a class="headerlink" href="#structured-output" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><p>LLM should produce output in a predefined format</p></li>
<li><p>different models implement different strategies</p></li>
<li><p>lower temperature is a good fit as it reduces the chance of LLM to produce invalid output</p></li>
<li><dl class="simple">
<dt><strong>Prompting</strong></dt><dd><ul class="simple">
<li><p>asking LLM to return output in desired format</p></li>
<li><p>not guaranteed for output to be in the format</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Tool Calling</strong></dt><dd><ul class="simple">
<li><p>available for LLMs fine-tuned to pick from a list of output schemas</p></li>
<li><p>need to give LLM a name, description, and schema for desired output format</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>JSON Mode</strong></dt><dd><ul class="simple">
<li><p>available in LLMs enforced to output a valid JSON document</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Joke</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">setup</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;The setup of the joke&quot;</span><span class="p">)</span>
    <span class="n">punchline</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;The punchline to the joke&quot;</span><span class="p">)</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4.1-mini&quot;</span><span class="p">)</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">Joke</span><span class="p">)</span>

<span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Tell me a joke about cats&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="streaming-output">
<h3>Streaming Output<a class="headerlink" href="#streaming-output" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><p>higher latency is acceptable if there is progress/intermediate output while the app is
still running</p></li>
<li><dl>
<dt><strong>Stream Modes in LangGraph</strong></dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">updates</span></code>: default mode</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">values</span></code>: yield current state of the graph every time it changes, each set of nodes
finishes executing</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">debug</span></code>: yields detailed events every time a graph changes</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">checkpoint</span></code> event: when a new checkpoint of current state is saved to the database</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">task</span></code> event: when a node is about to start running</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">task_result</span></code> events: when a node finishes running</p></li>
<li><p>stream modes can be combined</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;updates&quot;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Streaming Token-by-Token</strong></dt><dd><ul class="simple">
<li><p>useful for apps such as interactive chatbot</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">app</span><span class="o">.</span><span class="n">astream_events</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="s2">&quot;v2&quot;</span><span class="p">)</span>

<span class="k">async</span> <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">output</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">event</span><span class="p">[</span><span class="s2">&quot;event&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;on_chat_model_stream&quot;</span><span class="p">:</span>
        <span class="n">content</span> <span class="o">=</span> <span class="n">event</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">][</span><span class="s2">&quot;chunk&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>
        <span class="k">if</span> <span class="n">content</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="human-in-the-loop">
<h3>Human in the Loop<a class="headerlink" href="#human-in-the-loop" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><p>higher-agency architectures can have human intervention of interrupting, approving,
forking or undoing</p></li>
<li><p>store the state at the end of each step and combine the new input with the previous state
by using check pointer in graph</p></li>
<li><p>the graph remembering the previous state is the key to human-in-the-loop</p></li>
<li><p>Control Modes: interrupt, authorise, resume, restart, edit state, fork</p></li>
<li><p>combine different control modes to get better applications</p></li>
<li><dl>
<dt><strong>Interrupt</strong></dt><dd><ul class="simple">
<li><p>using an event or signal allows to control interruption from outside of the running
app</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">MemorySaver</span><span class="p">())</span>

<span class="n">event</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Event</span><span class="p">()</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>

<span class="k">async</span> <span class="k">with</span> <span class="n">aclosing</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">astream</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span> <span class="k">as</span> <span class="n">stream</span><span class="p">):</span>
    <span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">event</span><span class="o">.</span><span class="n">is_set</span><span class="p">():</span>
            <span class="k">break</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>

<span class="n">event</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Authorise</strong></dt><dd><ul class="simple">
<li><p>defined to give control to the user every time a specific node is about to be
called, usually used for tool confirmation</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">astream</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">interrupt_before</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;tools&quot;</span><span class="p">])</span>
<span class="k">async</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">output</span><span class="p">:</span>
    <span class="c1"># process output</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Resume</strong></dt><dd><ul class="simple">
<li><p>invoke the graph with null input to continue processing previous non-null input</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">astream</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">interrupt_before</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;tools&quot;</span><span class="p">])</span>
<span class="k">async</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">output</span><span class="p">:</span>
    <span class="c1"># process output</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Restart</strong></dt><dd><ul class="simple">
<li><p>invoke with new input to start a graph from the first node</p></li>
<li><p>will keep the current state, and merge it with new input</p></li>
<li><p>just change <code class="docutils literal notranslate"><span class="pre">thread_id</span></code> to start a new interaction from a blank state</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}}</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">astream</span><span class="p">(</span><span class="n">new_input</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="k">async</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">output</span><span class="p">:</span>
    <span class="c1"># process output</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Edit State</strong></dt><dd><ul class="simple">
<li><p>update the state of the graph before resuming</p></li>
<li><p>inspect the state first and update accordingly</p></li>
<li><p>will create a new checkpoint with the update</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">state</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">update</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">graph</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">update</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Fork</strong></dt><dd><ul class="simple">
<li><p>use the past states to get alternative answer</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">state</span> <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">get_state_history</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">history</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="double-texting-modes">
<h3>Double Texting Modes<a class="headerlink" href="#double-texting-modes" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>LLM may get new input before the previous one is processed</p></li>
<li><p>also called multitasking LLMs</p></li>
<li><dl class="simple">
<dt><strong>Refuse</strong></dt><dd><ul>
<li><p>simplest strategy to  reject concurrent inputs</p></li>
<li><p>concurrency management is handed off to the caller</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Handle Independently</strong></dt><dd><ul>
<li><p>treat new inputs as independent invocations, creating new threads and producing
output</p></li>
<li><p>user will receive as separate invocations, but can be scaled to large sizes</p></li>
<li><p>e.g. chatbot interacting with two different users concurrently</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Queue</strong></dt><dd><ul>
<li><p>inputs are queued and handled when current one is finished</p></li>
<li><p>can receive multiple concurrent requests, and will be handled sequentially</p></li>
<li><p>may take time to process the queue, which may grow unbounded and inputs can be stale</p></li>
<li><p>not useful when new inputs depend on previous answers</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Interrupt</strong></dt><dd><ul>
<li><p>stop current one and restart with the new input</p></li>
<li><p>previous input can be completely ignored</p></li>
<li><p>the completed state is kept but discard any pending state updates</p></li>
<li><p>keep the last completed step, along with current in-progress one</p></li>
<li><p>wait for current node to finish, but not the subsequent ones, save and interrupt</p></li>
<li><p>new input is handled quickly, reducing latency and stale outputs</p></li>
<li><p>the state needs to be designed to be stored partially</p></li>
<li><p>can have unpredictable final result as incomplete progress context might be used for
the new input</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Fork &amp; Merge</strong></dt><dd><ul>
<li><p>handle new input in parallel, forking the state of the thread, and merge the final
states</p></li>
<li><p>state needs to be designed to be merged without conflicts</p></li>
<li><p>e.g., use conflict-free replicated data types (CRDTs), conflict resolution
algorithms or manually resolve conflicts</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p><a class="reference external" href="#langchain">back to top</a></p>
</section>
</section>
<section id="deployment">
<h2>Deployment<a class="headerlink" href="#deployment" title="Link to this heading">#</a></h2>
<ul class="simple">
<li></li>
</ul>
<p><a class="reference external" href="#langchain">back to top</a></p>
</section>
<section id="prompting-basics">
<h2>Prompting Basics<a class="headerlink" href="#prompting-basics" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#llms">LLMs</a>, <a class="reference internal" href="#zero-shot-prompting">Zero-Shot Prompting</a>, <a class="reference internal" href="#few-shot-prompting">Few-Shot Prompting</a></p></li>
<li><p>prompts help the model understand context and generate relevant answers to queries</p></li>
<li><p>prompt engineering: adapting an existing LLM for specific task</p></li>
<li><p>Temperature: controls the randomness of LLM output</p></li>
<li><p>prompting techniques are most useful when combined with others</p></li>
</ul>
<section id="llms">
<h3>LLMs<a class="headerlink" href="#llms" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt><strong>Fine-Tuned</strong></dt><dd><ul>
<li><p>created by taking base LLMs, and further train on a proprietary dataset for a
specific task</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Instruction-Tuned</strong></dt><dd><ul>
<li><p>fine-tuned with task-specific datasets and RLHF</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Dialogue-Tuned</strong></dt><dd><ul>
<li><p>enhanced instruction-tuned LLMs</p></li>
<li><p>uses dialogue dataset and chat format</p></li>
<li><p>text is divided into parts associated with a role</p></li>
<li><p>System role: for instructions and framing the task</p></li>
<li><p>User role: actual task or question</p></li>
<li><p>Assistant role: for outputs of the model</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="zero-shot-prompting">
<h3>Zero-Shot Prompting<a class="headerlink" href="#zero-shot-prompting" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>simply telling the LLM to perform the desired task</p></li>
<li><p>usually work for simple questions</p></li>
<li><p>will need to iterate on prompts and responses to get a reliable system</p></li>
<li><dl class="simple">
<dt><strong>Chain-of-Thought</strong></dt><dd><ul>
<li><p>instructing the model to take time to think step by step</p></li>
<li><p>prepending the prompt with instructions for the LLM to describe how it could arrive
at the answer</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Retrieval-Augmented Generation</strong></dt><dd><ul>
<li><p>RAG: finding relevant context, and including them in the prompt</p></li>
<li><p>should be combined with CoT</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Tool Calling</strong></dt><dd><ul>
<li><p>prepending the prompt with a list of external functions LLM can use</p></li>
<li><p>developer should parse the output, and call functions that the LLM wants to use</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="few-shot-prompting">
<h3>Few-Shot Prompting<a class="headerlink" href="#few-shot-prompting" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>providing LLM with examples of other questions and correct answers</p></li>
<li><p>enables LLM to learn how to perform a new task without going through additional training
or fine-tuning</p></li>
<li><p>less powerful than fine-tuning, but more flexible and can do it at query time</p></li>
<li><dl class="simple">
<dt><strong>Static</strong></dt><dd><ul>
<li><p>include a predetermined list of a small number of examples in the prompt</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Dynamic</strong></dt><dd><ul>
<li><p>from a dataset of many examples, choose the most relevant ones for each new query</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p><a class="reference external" href="#langchain">back to top</a></p>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Kubernetes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Kubernetes</p>
      </div>
    </a>
    <a class="right-next"
       href="Linux.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Linux</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basics">Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interfaces">Interfaces</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-output">LLM Output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#component-composition">Component Composition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag">RAG</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-indexing">Data Indexing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing-optimisations">Indexing Optimisations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-transformation">Query Transformation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-routing">Query Routing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-construction">Query Construction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#langgraph">LangGraph</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph">Graph</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory">Memory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiactor">Multiactor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chat-history">Chat History</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#subgraphs">Subgraphs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#langgraph-platform">LangGraph Platform</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-models">Data Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#features">Features</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cognitive-architectures">Cognitive Architectures</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#degree-of-autonomy">Degree of Autonomy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-call-architecture">LLM Call Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-architecture">Chain Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#router-architecture">Router Architecture</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#agent-architecture">Agent Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-agent">Standard Agent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#always-tool-calling-first">Always Tool Calling First</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#managing-multiple-tools">Managing Multiple Tools</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reflection">Reflection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-agent">Multi-agent</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-patterns">LLM Patterns</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#structured-output">Structured Output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#streaming-output">Streaming Output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#human-in-the-loop">Human in the Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#double-texting-modes">Double Texting Modes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deployment">Deployment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompting-basics">Prompting Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llms">LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-prompting">Zero-Shot Prompting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#few-shot-prompting">Few-Shot Prompting</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Myat Kyaw Tun
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025, Myat Kyaw Tun.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>