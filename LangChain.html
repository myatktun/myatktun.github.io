
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>LangChain &#8212; Notes</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'LangChain';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Linux" href="Linux.html" />
    <link rel="prev" title="Kubernetes" href="Kubernetes.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Notes</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="AITools.html">AI Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="AWS.html">AWS</a></li>
<li class="toctree-l1"><a class="reference internal" href="ArmArchitecture.html">Arm Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="Assembly.html">Assembly</a></li>
<li class="toctree-l1"><a class="reference internal" href="BackendEngineering.html">Backend Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="C.html">C</a></li>
<li class="toctree-l1"><a class="reference internal" href="C%2B%2B.html">C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="CSharp.html">C# #</a></li>
<li class="toctree-l1"><a class="reference internal" href="CTF.html">CTF</a></li>
<li class="toctree-l1"><a class="reference internal" href="CompTIA.html">CompTIA Cert Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="Compilers.html">Compilers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ComputerGraphics.html">Computer Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="ComputerVision.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="ComputingSystems.html">Computing Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="CyberSecurity.html">Cyber Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="DataStructure%26Algorithms.html">Data Structure &amp; Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="DatabaseEngineering.html">Database Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="DeepLearning.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="DesignPatterns.html">Design Patterns</a></li>
<li class="toctree-l1"><a class="reference internal" href="Docker.html">Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="ECS.html">ECS</a></li>
<li class="toctree-l1"><a class="reference internal" href="FFmpeg.html">FFmpeg</a></li>
<li class="toctree-l1"><a class="reference internal" href="Git.html">Git</a></li>
<li class="toctree-l1"><a class="reference internal" href="Helm.html">Helm</a></li>
<li class="toctree-l1"><a class="reference internal" href="Istio.html">Istio</a></li>
<li class="toctree-l1"><a class="reference internal" href="JDBC.html">JDBC</a></li>
<li class="toctree-l1"><a class="reference internal" href="Java.html">Java</a></li>
<li class="toctree-l1"><a class="reference internal" href="Jenkins.html">Jenkins</a></li>
<li class="toctree-l1"><a class="reference internal" href="Kubernetes.html">Kubernetes</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="Linux.html">Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="MachineLearning.html">Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Microservices.html">Microservices</a></li>
<li class="toctree-l1"><a class="reference internal" href="MongoDB.html">MongoDB</a></li>
<li class="toctree-l1"><a class="reference internal" href="NetworkProgramming.html">Network Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="NodeJS.html">Node.js</a></li>
<li class="toctree-l1"><a class="reference internal" href="OpenAPI.html">OpenAPI</a></li>
<li class="toctree-l1"><a class="reference internal" href="OperatingSystem.html">Operating System</a></li>
<li class="toctree-l1"><a class="reference internal" href="POSIXThreads.html">POSIX Threads</a></li>
<li class="toctree-l1"><a class="reference internal" href="PenetrationTesting.html">Penetration Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="ProgramDesign.html">Program Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="ProjectManagement.html">Project Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="Prometheus.html">Prometheus</a></li>
<li class="toctree-l1"><a class="reference internal" href="Python.html">Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="React.html">React</a></li>
<li class="toctree-l1"><a class="reference internal" href="Redis.html">Redis</a></li>
<li class="toctree-l1"><a class="reference internal" href="SFML.html">SFML</a></li>
<li class="toctree-l1"><a class="reference internal" href="SQL.html">SQL</a></li>
<li class="toctree-l1"><a class="reference internal" href="StrategicPlanning.html">Strategic Planning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Terraform.html">Terraform</a></li>
<li class="toctree-l1"><a class="reference internal" href="TypeScript.html">TypeScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="UIUX.html">UI/UX</a></li>
<li class="toctree-l1"><a class="reference internal" href="Vagrant.html">Vagrant</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/LangChain.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LangChain</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basics">Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interfaces">Interfaces</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-output">LLM Output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#component-composition">Component Composition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag">RAG</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-indexing">Data Indexing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing-optimisations">Indexing Optimisations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-transformation">Query Transformation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-routing">Query Routing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-construction">Query Construction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#langgraph">LangGraph</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph">Graph</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory">Memory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiactor">Multiactor</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompting-basics">Prompting Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llms">LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-prompting">Zero-Shot Prompting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#few-shot-prompting">Few-Shot Prompting</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="langchain">
<h1>LangChain<a class="headerlink" href="#langchain" title="Link to this heading">#</a></h1>
<ol class="arabic simple">
<li><p><a class="reference internal" href="#basics">Basics</a></p></li>
<li><p><a href="#id5"><span class="problematic" id="id6">`Data Manipulation`_</span></a></p></li>
<li><p><a class="reference internal" href="#rag">RAG</a></p></li>
<li><p><a class="reference internal" href="#langgraph">LangGraph</a></p></li>
<li><p><a class="reference internal" href="#prompting-basics">Prompting Basics</a></p></li>
</ol>
<p><a class="reference external" href="#langchain">back to top</a></p>
<section id="basics">
<h2>Basics<a class="headerlink" href="#basics" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#interfaces">Interfaces</a>, <a class="reference internal" href="#llm-output">LLM Output</a>, <a class="reference internal" href="#component-composition">Component Composition</a></p></li>
<li><p>LangChain provides abstractions for each major prompting technique, utilising Python and
JavaScript for wrappers</p></li>
<li><p>has integrations with commercial and open source LLM providers</p></li>
<li><p>prompt templates enable to reuse prompts more than once, and store them in the LangChain Hub</p></li>
</ul>
<section id="interfaces">
<h3>Interfaces<a class="headerlink" href="#interfaces" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><dl class="simple">
<dt><strong>Chat Model</strong></dt><dd><ul class="simple">
<li><p>LLM providers like OpenAI differentiate messages sent to and from the model into
roles</p></li>
<li><p>System role: for instructions the model should use to answer a user question</p></li>
<li><p>User role: for user’s query and other content produced by the user</p></li>
<li><p>Assistant role: for content generated by the model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">temperature</span></code>: controls sampling algorithm, lower values produce more predictable
outputs, and higher values do better for creative tasks</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_tokens</span></code>: limits the size and cost of output</p></li>
<li><p>chat models make use of different types of chat message interfaces associated with
each role</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HumanMessage</span></code>: message sent from human, user role</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">AIMessage</span></code>: message sent from AI, assistant role</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SystemMessage</span></code>: message setting the instructions for AI, system role</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ChatMessage</span></code>: message for arbitrary setting of role</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>LLM</strong></dt><dd><ul class="simple">
<li><p>takes a string input, send it to the model provider, and returns the model
prediction as output</p></li>
<li><p>LangChain interact with LLMs using function calling or traditional prompting</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Prompt Template</strong></dt><dd><ul class="simple">
<li><p>allow to construct prompts with dynamic inputs</p></li>
<li><p>use <code class="docutils literal notranslate"><span class="pre">ChatPromptTemplate</span></code> for AI chat applications</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span><span class="p">,</span> <span class="n">ChatPromptTemplate</span>

<span class="n">template</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;Answer the question based on the context</span>
<span class="s2">below.</span>

<span class="s2">Context: </span><span class="si">{context}</span>
<span class="s2">Question: </span><span class="si">{question}</span>
<span class="s2">Answer: &quot;&quot;&quot;</span><span class="p">)</span>

<span class="c1"># Using ChatPromptTemplate will associate with roles</span>
<span class="n">template</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;Answer the question base on the context below.&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;human&#39;</span><span class="p">,</span> <span class="s1">&#39;Context: </span><span class="si">{context}</span><span class="s1">&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;human&#39;</span><span class="p">,</span> <span class="s1">&#39;Question: </span><span class="si">{question}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">template</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span>
    <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="s2">&quot;This is context&quot;</span><span class="p">,</span>
    <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;Question based on context?&quot;</span>
<span class="p">})</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Runnable</strong></dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">invoke()</span></code>: single input to output</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch()</span></code>:  multiple inputs to multiple outputs</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stream()</span></code>: stream output from a single input as it’s produced</p></li>
<li><p>each method has <code class="docutils literal notranslate"><span class="pre">asyncio</span></code> equivalents</p></li>
<li><p>utilities for retries, fallbacks, schemas, and runtime configurability are available</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s1">&#39;Hi there!&#39;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">batch</span><span class="p">([</span><span class="s1">&#39;Hi there!&#39;</span><span class="p">,</span> <span class="s1">&#39;Bye!&#39;</span><span class="p">])</span>

<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="s1">&#39;Bye!&#39;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="llm-output">
<h3>LLM Output<a class="headerlink" href="#llm-output" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><p>LLM can generate specific output format, such as JSON, XML, CSV</p></li>
<li><dl>
<dt><strong>JSON Output</strong></dt><dd><ul class="simple">
<li><p>need to define schema using Pydantic, and include it in the prompt</p></li>
<li><p>schema is converted to <code class="docutils literal notranslate"><span class="pre">JSONSchema</span></code> object, and used to validate the output from LLM</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">AnswerWithJustification</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;An answer to the user&#39;s question along with justification for the answer. &#39;&#39;&#39;</span>
    <span class="n">answer</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">justification</span><span class="p">:</span> <span class="nb">str</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;supported_model&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">structured_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">AnswerWithJustification</span><span class="p">)</span>
<span class="n">msg</span> <span class="o">=</span> <span class="n">structured_llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;Question&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Output Parsers</strong></dt><dd><ul class="simple">
<li><p>classes to structure LLM responses</p></li>
<li><p>can be used to provide output format instructions in the prompt</p></li>
<li><p>textual output can be rendered to a more structured format</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">CommaSeparatedListOutputParser</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">CommaSeparatedListOutputParser</span><span class="p">()</span>
<span class="n">items</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;apple, banana, cherry&quot;</span><span class="p">)</span> <span class="c1"># [&#39;apple&#39;, &#39;banana&#39;, &#39;cherry&#39;]</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="component-composition">
<h3>Component Composition<a class="headerlink" href="#component-composition" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><dl>
<dt><strong>Imperative Composition</strong></dt><dd><ul class="simple">
<li><p>calling components directly, e.g. <code class="docutils literal notranslate"><span class="pre">model.invoke()</span></code></p></li>
<li><p>Parallel execution: threads or coroutines in Python, and <code class="docutils literal notranslate"><span class="pre">Promise.all</span></code> in JavaScript</p></li>
<li><p>Streaming: using <code class="docutils literal notranslate"><span class="pre">yield</span></code></p></li>
<li><p>Async execution: with async functions</p></li>
<li><p>useful for writing custom logic</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">template</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;You are a helpful assistant.&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;human&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{question}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span>

<span class="c1"># combine components in a function</span>
<span class="nd">@chain</span>
<span class="k">def</span> <span class="nf">chatbot</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">template</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">token</span>

<span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">chatbot</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;Question&quot;</span><span class="p">}):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">part</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Declarative Composition</strong></dt><dd><ul class="simple">
<li><p>using LCEL (LangChain Expression Language)</p></li>
<li><p>LCEL compositions are compiled to an optimised execution plan</p></li>
<li><p>Streaming, Parallel and Async executions are automatic</p></li>
<li><p>useful for assembling existing components with limited customisation</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">template</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;You are a helpful assistant.&#39;</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;human&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{question}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span>

<span class="c1"># combine components with | operator</span>
<span class="n">chatbot</span> <span class="o">=</span> <span class="n">template</span> <span class="o">|</span> <span class="n">model</span>

<span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">chatbot</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="s2">&quot;Question&quot;</span><span class="p">}):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">part</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p><a class="reference external" href="#langchain">back to top</a></p>
</section>
</section>
<section id="rag">
<h2>RAG<a class="headerlink" href="#rag" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#data-indexing">Data Indexing</a>, <a class="reference internal" href="#indexing-optimisations">Indexing Optimisations</a>, <a class="reference internal" href="#query-transformation">Query Transformation</a>, <a class="reference internal" href="#query-routing">Query Routing</a></p></li>
<li><p><a class="reference internal" href="#query-construction">Query Construction</a></p></li>
</ul>
<section id="data-indexing">
<h3>Data Indexing<a class="headerlink" href="#data-indexing" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><p>indexing is a technique to enhance LLM output by providing context from external sources</p></li>
<li><p>processing external data source, and storing embeddings in a vector store</p></li>
<li><p>embed a user’s query, retrieve similar documents, and passing them as context to the
prompt</p></li>
<li><p>Retrieving: getting relevant embeddings and data stored in the vector store based on
user’s query</p></li>
<li><p>Generation: synthesising original prompt with the retrieved relevant documents</p></li>
<li><p>Ingestion: converting documents into embeddings, and storing in vector store</p></li>
<li><p>Context Window: size of input and output tokens LLMs and embedding models can handle</p></li>
<li><dl class="simple">
<dt><strong>Document Loader</strong></dt><dd><ul class="simple">
<li><p>can load files such as txt, csv, json, Markdown, and integrate with platforms such
as Slack and Notion</p></li>
<li><p>can use <code class="docutils literal notranslate"><span class="pre">WebBaseLoader</span></code> to load HTML, or <code class="docutils literal notranslate"><span class="pre">PyPDFLoader</span></code> with <code class="docutils literal notranslate"><span class="pre">pypdf</span></code> package</p></li>
<li><p>loaded data is stored in <code class="docutils literal notranslate"><span class="pre">Document</span></code> class, and need to be split into chunks
semantically</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt><code class="docutils literal notranslate"><span class="pre">RecursiveCharacterTextSplitter</span></code></dt><dd><ul class="simple">
<li><p>can split text based on a list of separators in order</p></li>
<li><p>default separator order: <code class="docutils literal notranslate"><span class="pre">\n\n</span></code> (paragraph), <code class="docutils literal notranslate"><span class="pre">\n</span></code> (line), space (word)</p></li>
<li><p>split paragraphs that are within the chunk size</p></li>
<li><p>for paragraphs longer than the chunk size, split by the next separator</p></li>
<li><p>each chunk is a <code class="docutils literal notranslate"><span class="pre">Document</span></code> with metadata of the original document</p></li>
<li><p>can use for others, such as code languages and Markdown, with relevant separators</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_text_splitters</span> <span class="kn">import</span> <span class="n">Language</span><span class="p">,</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">TextLoader</span>

<span class="n">loader</span> <span class="o">=</span> <span class="n">TextLoader</span><span class="p">(</span><span class="s2">&quot;./main.py&quot;</span><span class="p">)</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="n">splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="o">.</span><span class="n">from_language</span><span class="p">(</span>
    <span class="n">language</span><span class="o">=</span><span class="n">Language</span><span class="o">.</span><span class="n">PYTHON</span><span class="p">,</span>
    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>

<span class="n">python_docs</span> <span class="o">=</span> <span class="n">splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Embedding</strong></dt><dd><ul class="simple">
<li><p>converting text to numbers that cannot be used to recover original text</p></li>
<li><p>both text and numerals are stored since it is a lossy representation</p></li>
<li><p>Dense embeddings: low-dimensional vectors with mostly non-zero values</p></li>
<li><p>Sparse embeddings: high-dimensional vectors with mostly zero values</p></li>
<li><p>never combine embeddings from different models</p></li>
<li><p>words or sentences that are close in meaning should be closer in semantic dimension</p></li>
<li><p>cosine similarity is usually used for degree of similarity</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Embeddings</span></code> class interfaces with text embedding models, and generate vector
representations</p></li>
<li><p>can embed documents and query</p></li>
<li><p>embedding multiple documents at the same time is more efficient</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">([</span>
    <span class="s2">&quot;Hi there!&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Oh, hello!&quot;</span><span class="p">,</span>
    <span class="s2">&quot;What&#39;s your name?&quot;</span><span class="p">,</span>
    <span class="s2">&quot;My friends call me World&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Hello World!&quot;</span>
<span class="p">])</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Vector Store</strong></dt><dd><ul class="simple">
<li><p>database to store vectors and perform complex calculations</p></li>
<li><p>handle unstructured data, including text and images</p></li>
<li><p>has capabilities such as multi-tenancy and metadata filtering</p></li>
<li><p>PostgreSQL can be used as vector store with <code class="docutils literal notranslate"><span class="pre">pgvector</span></code> extension</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">add_documents()</span></code>: create embeddings for each document, and store them</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">connection</span> <span class="o">=</span> <span class="s1">&#39;PostgreSQL_Connection&#39;</span>

<span class="n">db</span> <span class="o">=</span> <span class="n">PGVector</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embeddings_model</span><span class="p">,</span> <span class="n">connection</span><span class="o">=</span><span class="n">connection</span><span class="p">)</span>

<span class="n">db</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="s2">&quot;query&quot;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>

<span class="n">db</span><span class="o">.</span><span class="n">add_documents</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">Document</span><span class="p">(</span>
            <span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;Content&quot;</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;value&quot;</span><span class="p">}</span>
        <span class="p">)</span>
    <span class="p">],</span>
    <span class="n">ids</span><span class="o">=</span><span class="n">ids</span>
<span class="p">)</span>

<span class="n">db</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">])</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Indexing API</strong></dt><dd><ul class="simple">
<li><p>uses <code class="docutils literal notranslate"><span class="pre">RecordManager</span></code> to track document writes into the vector store</p></li>
<li><p>stores document hash, write time, and source ID</p></li>
<li><p>provides cleanup modes to delete existing documents</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code>: manual clean up of old content</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Icremental</span></code> &amp; <code class="docutils literal notranslate"><span class="pre">full</span></code>: delete previous versions if content of the source document or
derived ones change</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Full</span></code>: delete any documents not included in documents currently being indexed</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.indexes</span> <span class="kn">import</span> <span class="n">SQLRecordManager</span><span class="p">,</span> <span class="n">index</span>

<span class="n">collection_name</span> <span class="o">=</span> <span class="s2">&quot;my_docs&quot;</span>
<span class="n">embeddings_model</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">)</span>
<span class="n">namespace</span> <span class="o">=</span> <span class="s2">&quot;my_docs_namespace&quot;</span>

<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">PGVector</span><span class="p">(</span>
    <span class="n">embeddings</span><span class="o">=</span><span class="n">embeddings_model</span><span class="p">,</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span>
    <span class="n">connection</span><span class="o">=</span><span class="n">connection</span><span class="p">,</span>
    <span class="n">use_jsonb</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">record_manager</span> <span class="o">=</span> <span class="n">SQLRecordManager</span><span class="p">(</span>
    <span class="n">namespace</span><span class="p">,</span>
    <span class="n">db_url</span><span class="o">=</span><span class="n">connection</span>
<span class="p">)</span>

<span class="n">record_manager</span><span class="o">.</span><span class="n">create_schema</span><span class="p">()</span>

<span class="n">docs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">Document</span><span class="p">(</span>
        <span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;content 1&quot;</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;source_1.txt&quot;</span><span class="p">}</span>
    <span class="p">),</span>
    <span class="n">Document</span><span class="p">(</span>
        <span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;content 2&quot;</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;source_2.txt&quot;</span><span class="p">}</span>
    <span class="p">)</span>
<span class="p">]</span>

<span class="n">index_1</span> <span class="o">=</span> <span class="n">index</span><span class="p">(</span>
    <span class="n">docs</span><span class="p">,</span>
    <span class="n">record_manager</span><span class="p">,</span>
    <span class="n">vectorstore</span><span class="p">,</span>
    <span class="n">cleanup</span><span class="o">=</span><span class="s2">&quot;incremental&quot;</span><span class="p">,</span>
    <span class="n">source_id_key</span><span class="o">=</span><span class="s2">&quot;source&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Index attempt 1: &quot;</span><span class="p">,</span> <span class="n">index_1</span><span class="p">)</span>

<span class="n">index_2</span> <span class="o">=</span> <span class="n">index</span><span class="p">(</span>
    <span class="n">docs</span><span class="p">,</span>
    <span class="n">record_manager</span><span class="p">,</span>
    <span class="n">vectorstore</span><span class="p">,</span>
    <span class="n">cleanup</span><span class="o">=</span><span class="s2">&quot;incremental&quot;</span><span class="p">,</span>
    <span class="n">source_id_key</span><span class="o">=</span><span class="s2">&quot;source&quot;</span>
<span class="p">)</span>

<span class="c1"># attempting to index again will not add the documents</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Index attempt 2: &quot;</span><span class="p">,</span> <span class="n">index_2</span><span class="p">)</span>

<span class="n">docs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span> <span class="o">=</span> <span class="s2">&quot;modified&quot;</span>

<span class="n">index_3</span> <span class="o">=</span> <span class="n">index</span><span class="p">(</span>
    <span class="n">docs</span><span class="p">,</span>
    <span class="n">record_manager</span><span class="p">,</span>
    <span class="n">vectorstore</span><span class="p">,</span>
    <span class="n">cleanup</span><span class="o">=</span><span class="s2">&quot;incremental&quot;</span><span class="p">,</span>
    <span class="n">source_id_key</span><span class="o">=</span><span class="s2">&quot;source&quot;</span>
<span class="p">)</span>

<span class="c1"># new version is written, and all old versions sharing the same source are deleted</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Index attempt 3: &quot;</span><span class="p">,</span> <span class="n">index_3</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="indexing-optimisations">
<h3>Indexing Optimisations<a class="headerlink" href="#indexing-optimisations" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><dl>
<dt><code class="docutils literal notranslate"><span class="pre">MultiVectorRetriever</span></code></dt><dd><ul class="simple">
<li><p>decouple documents to use for answer synthesis</p></li>
<li><p>e.g. in a document of text and tables, embed summaries of table elements with an id
reference to the full raw table, which is stored in a separate Docstore</p></li>
<li><p>enables to provide the model with full context to answer user’s question</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain.storage</span> <span class="kn">import</span> <span class="n">InMemoryStore</span>
<span class="kn">from</span> <span class="nn">langchain_postgres</span> <span class="kn">import</span> <span class="n">PGVector</span>
<span class="kn">from</span> <span class="nn">langchain.retrievers.multi_vector</span> <span class="kn">import</span> <span class="n">MultiVectorRetriever</span>

<span class="c1"># load the document, split, create embeddings and LLM model</span>

<span class="n">prompt_text</span> <span class="o">=</span> <span class="s2">&quot;Summarize the following document:</span><span class="se">\n\n</span><span class="si">{doc}</span><span class="s2">&quot;</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">prompt_text</span><span class="p">)</span>

<span class="n">summarise_chain</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;doc&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">page_content</span><span class="p">}</span> <span class="o">|</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>

<span class="n">summaries</span> <span class="o">=</span> <span class="n">summarise_chain</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">chunks</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;max_concurrency&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">})</span>

<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">PGVector</span><span class="p">(</span>
    <span class="n">embeddings</span><span class="o">=</span><span class="n">embeddings_model</span><span class="p">,</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span>
    <span class="n">connection</span><span class="o">=</span><span class="n">connection</span><span class="p">,</span>
    <span class="n">use_jsonb</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="c1"># for parent documents</span>
<span class="n">store</span> <span class="o">=</span> <span class="n">InMemoryStore</span><span class="p">()</span>
<span class="n">id_key</span> <span class="o">=</span> <span class="s2">&quot;doc_id&quot;</span>

<span class="n">retriever</span> <span class="o">=</span> <span class="n">MultiVectorRetriever</span><span class="p">(</span>
    <span class="n">vectorstore</span><span class="o">=</span><span class="n">vectorstore</span><span class="p">,</span>
    <span class="n">docstore</span><span class="o">=</span><span class="n">store</span><span class="p">,</span>
    <span class="n">id_key</span><span class="o">=</span><span class="n">id_key</span>
<span class="p">)</span>

<span class="n">doc_ids</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">]</span>

<span class="n">summary_docs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="n">id_key</span><span class="p">:</span> <span class="n">doc_ids</span><span class="p">[</span><span class="n">i</span><span class="p">]})</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">summaries</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">retriever</span><span class="o">.</span><span class="n">vectorstore</span><span class="o">.</span><span class="n">add_documents</span><span class="p">(</span><span class="n">summary_docs</span><span class="p">)</span>

<span class="c1"># store the original documents, linked to summaries via doc_ids</span>
<span class="n">retriever</span><span class="o">.</span><span class="n">docstore</span><span class="o">.</span><span class="n">mset</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">doc_ids</span><span class="p">,</span> <span class="n">chunks</span><span class="p">)))</span>

<span class="c1"># vector store retrieves the summaries</span>
<span class="n">sub_docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="o">.</span><span class="n">vectorstore</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span>
    <span class="s2">&quot;topic&quot;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># retriever return the larger source document chunks</span>
<span class="n">retrieved_docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;topic&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>RAPTOR</strong></dt><dd><ul class="simple">
<li><p>Recursive Abstractive Processing for Tree-Organised Retrieval</p></li>
<li><p>creating document summaries for higher-level concepts, embedding and clustering them
and summarising each cluster</p></li>
<li><p>recursively done to produce a tree of higher-level summaries</p></li>
<li><p>then the summaries and initial documents are indexed together</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>ColBERT</strong></dt><dd><ul class="simple">
<li><p>effective embeddings approach for better retrieval</p></li>
<li><p>generate contextual embeddings for each token in the document and query</p></li>
<li><p>calculate and score similarity between each query token and all document tokens</p></li>
<li><p>sum the max similarity score of each query embedding to any of the document
embeddings to get a score for each document</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="query-transformation">
<h3>Query Transformation<a class="headerlink" href="#query-transformation" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><p>modifying user input to be more or less abstract to generate accurate LLM output</p></li>
<li><dl>
<dt><strong>Rewrite-Retrieve-Read</strong></dt><dd><ul class="simple">
<li><p>prompts the LLM to rewrite the user’s query before performing retrieval</p></li>
<li><p>remove irrelevant information in the query with the help of LLM</p></li>
<li><p>but will add additional latency in the chain due to more LLM calls</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rewrite_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Provide a better search query for web search engine to answer the given</span>
<span class="s2">question, end the queries with &#39;**&#39;. Question: </span><span class="si">{x}</span><span class="s2"> Answer:</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">parse_rewriter_output</span><span class="p">(</span><span class="n">message</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;&quot;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s2">&quot;**&quot;</span><span class="p">)</span>

<span class="n">rewriter</span> <span class="o">=</span> <span class="n">rewrite_prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parse_rewriter_output</span>

<span class="nd">@chain</span>
<span class="k">def</span> <span class="nf">qa_rrr</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="n">new_query</span> <span class="o">=</span> <span class="n">rewriter</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">new_query</span><span class="p">)</span>
    <span class="n">formatted</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="nb">input</span><span class="p">})</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">formatted</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">answer</span>

<span class="n">qa_rrr</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Query with irrelevant information&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Multi-Query Retrieval</strong></dt><dd><ul class="simple">
<li><p>tell LLM to generate multiple queries based on the user’s initial one</p></li>
<li><p>each query is retrieved in parallel and inserted as prompt context for final output</p></li>
<li><p>useful when a single question may rely on multiple perspectives for an answer</p></li>
<li><p>should deduplicate documents as single retriever is used with multiple queries</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">perspectives_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are an AI language model assistant. Your task is to generate five</span>
<span class="s2">different versions of the given user question to retrieve relvant documents</span>
<span class="s2">from a vector database. By generating multiple perspectives on the user</span>
<span class="s2">question, your goal is to help the user overcome come of the limitations of</span>
<span class="s2">the distance-based similarity search. Provide these alternative questions</span>
<span class="s2">separated by newlines. Original question: </span><span class="si">{question}</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">parse_queries_output</span><span class="p">(</span><span class="n">message</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">query_gen</span> <span class="o">=</span> <span class="n">perspectives_prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parse_queries_output</span>

<span class="k">def</span> <span class="nf">get_unique_union</span><span class="p">(</span><span class="n">document_lists</span><span class="p">):</span>
    <span class="n">deduped_docs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">doc</span><span class="o">.</span><span class="n">page_content</span><span class="p">:</span> <span class="n">doc</span>
        <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">document_lists</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">sublist</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">deduped_docs</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

<span class="n">retrieval_chain</span> <span class="o">=</span> <span class="n">query_gen</span> <span class="o">|</span> <span class="n">retriever</span><span class="o">.</span><span class="n">batch</span> <span class="o">|</span> <span class="n">get_unique_union</span>

<span class="nd">@chain</span>
<span class="k">def</span> <span class="nf">multi_query_qa</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="n">retrieval_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">formatted</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="nb">input</span><span class="p">})</span>
    <span class="n">ans</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">formatted</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ans</span>

<span class="n">multi_query_qa</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Question&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>RAG-Fusion</strong></dt><dd><ul class="simple">
<li><p>similar to the Multi-Query retrieval</p></li>
<li><p>retrieved documents are re-ranked at the final step with RRF (Reciprocal Rank
Fusion) algorithm, pulling the most relevant documents to the top</p></li>
<li><p>RRF is ideal for combining results from queries with different scales or
distributions of scores</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># def multi_query_qa()</span>

<span class="n">prompt_rag_fusion</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are a helpful assistant that generates multiple search queries based on</span>
<span class="s2">a single input query.</span><span class="se">\n</span>
<span class="s2">Generate multiple search queries related to: </span><span class="si">{question}</span><span class="s2"> </span><span class="se">\n</span>
<span class="s2">Output (4 queries):</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">query_gen</span> <span class="o">=</span> <span class="n">prompt_rag_fusion</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parse_queries_output</span>

<span class="n">retrieval_chain</span> <span class="o">=</span> <span class="n">query_gen</span> <span class="o">|</span> <span class="n">retriever</span><span class="o">.</span><span class="n">batch</span> <span class="o">|</span> <span class="n">reciprocal_rank_fusion</span>

<span class="n">multi_query_qa</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Question&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">reciprocal_rank_fusion</span><span class="p">(</span><span class="n">results</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="mi">60</span><span class="p">):</span>
    <span class="n">fused_scores</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">docs</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
            <span class="n">doc_str</span> <span class="o">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">page_content</span>
            <span class="k">if</span> <span class="n">doc_str</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">fused_scores</span><span class="p">:</span>
                <span class="n">fused_scores</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">documents</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span> <span class="o">=</span> <span class="n">doc</span>

            <span class="n">fused_scores</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="n">k</span><span class="p">)</span>

    <span class="n">reranked_doc_strs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
        <span class="n">fused_scores</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">fused_scores</span><span class="p">[</span><span class="n">d</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">documents</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span> <span class="k">for</span> <span class="n">doc_str</span> <span class="ow">in</span> <span class="n">reranked_doc_strs</span><span class="p">]</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>HyDE</strong></dt><dd><ul class="simple">
<li><p>Hypothetical Document Embeddings</p></li>
<li><p>create hypothetical document based on user’s query, embed it, and retrieve relevant
documents based on vector similarity</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prompt_hyde</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Please write a passage to answer the question.</span><span class="se">\n</span>
<span class="s2">Question: </span><span class="si">{question}</span><span class="s2"> </span><span class="se">\n</span>
<span class="s2">Passage:</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Answer the following question based on this context:</span>

<span class="si">{context}</span>

<span class="s2">Question: </span><span class="si">{question}</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">generate_doc</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>

<span class="n">retrieval_chain</span> <span class="o">=</span> <span class="n">generate_doc</span> <span class="o">|</span> <span class="n">retriever</span>

<span class="nd">@chain</span>
<span class="k">def</span> <span class="nf">qa</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="n">retrieval_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">formatted</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">docs</span><span class="p">,</span> <span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="nb">input</span><span class="p">})</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">formatted</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">answer</span>

<span class="n">qa</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Question&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="query-routing">
<h3>Query Routing<a class="headerlink" href="#query-routing" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><p>to forward user’s query to the relevant data source</p></li>
<li><dl>
<dt><strong>Logical Routing</strong></dt><dd><ul class="simple">
<li><p>let LLM decide which data source to apply based on the query</p></li>
<li><p>function-calling models are used to help classify each query</p></li>
<li><p>need to define a schema that the model can use to generate arguments of a function
based on the query</p></li>
<li><p>extracted data source can be passed into other functions for additional logic</p></li>
<li><p>suitable when a defined list of data sources is available</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Literal</span>

<span class="k">class</span> <span class="nc">RouteQuery</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">datasource</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;source_1&quot;</span><span class="p">,</span> <span class="s2">&quot;source_2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;Given a user question, choose which datasource would be</span>
<span class="s2">        most relevant for answering their question</span>
<span class="s2">        &quot;&quot;&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">choose_route</span><span class="p">(</span><span class="n">result</span><span class="p">):</span>
    <span class="k">if</span> <span class="s2">&quot;source_1&quot;</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">datasource</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
        <span class="k">return</span> <span class="s2">&quot;chain for source_1&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;chain for source_2&quot;</span>

<span class="n">structured_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">RouteQuery</span><span class="p">)</span>

<span class="n">system</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;You are an expert at routing a user question to the appropriate</span>
<span class="s2">data source.</span>

<span class="s2">Based on the programming language the question is referring to, route it to</span>
<span class="s2">the relevant data source.</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="n">system</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{question}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">router</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">structured_llm</span>

<span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;Question&quot;</span>

<span class="c1"># chaining for additional logic</span>
<span class="n">full_chain</span> <span class="o">=</span> <span class="n">router</span> <span class="o">|</span> <span class="n">RunnableLambda</span><span class="p">(</span><span class="n">choose_route</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">full_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Semantic Routing</strong></dt><dd><ul class="simple">
<li><p>embedding various prompts of various data sources with the query, and doing vector
similarity search for the most similar prompt</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain.utils.math</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>

<span class="n">template_1</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Template 1</span>
<span class="s2">Here is a question:</span>
<span class="si">{query}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">template_2</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Template 2</span>
<span class="s2">Here is a question:</span>
<span class="si">{query}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">prompt_templates</span> <span class="o">=</span> <span class="p">[</span><span class="n">template_1</span><span class="p">,</span> <span class="n">template_2</span><span class="p">]</span>
<span class="n">prompt_embeddings</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">(</span><span class="n">prompt_templates</span><span class="p">)</span>

<span class="nd">@chain</span>
<span class="k">def</span> <span class="nf">prompt_router</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
    <span class="n">query_embedding</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">([</span><span class="n">query_embedding</span><span class="p">],</span> <span class="n">prompt_embeddings</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">most_similar</span> <span class="o">=</span> <span class="n">prompt_templates</span><span class="p">[</span><span class="n">similarity</span><span class="o">.</span><span class="n">argmax</span><span class="p">()]</span>
    <span class="k">return</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">most_similar</span><span class="p">)</span>

<span class="n">semantic_router</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">prompt_router</span>
    <span class="o">|</span> <span class="n">llm</span>
    <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">semantic_router</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Question&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="query-construction">
<h3>Query Construction<a class="headerlink" href="#query-construction" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul>
<li><p>convert natural language query into language of database or data source</p></li>
<li><dl>
<dt><strong>Text-to-Metadata Filter</strong></dt><dd><ul class="simple">
<li><p>can attach metadata key-value pairs to vectors in an index during embedding process</p></li>
<li><p>filter expressions will be used during query</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SelfQueryRetriever</span></code> uses LLM to extract and execute relevant metadata filters based
on user’s query and predefined metadata schema</p></li>
<li><p>retriever will send query generation prompt, parse metadata filter and rewritten
query, convert the metadata filter for vector store, and run similarity search
against the vector store</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.chains.query_constructor.schema</span> <span class="kn">import</span> <span class="n">AttributeInfo</span>
<span class="kn">from</span> <span class="nn">langchain.retrievers.self_query.base</span> <span class="kn">import</span> <span class="n">SelfQueryRetriever</span>

<span class="n">fields</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">AttributeInfo</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;NAME&quot;</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;DESC&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string or list[string]&quot;</span>
    <span class="p">),</span>
<span class="p">]</span>

<span class="n">description</span> <span class="o">=</span> <span class="s2">&quot;DESC&quot;</span>

<span class="n">retriever</span> <span class="o">=</span> <span class="n">SelfQueryRetriever</span><span class="o">.</span><span class="n">from_llm</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">db</span><span class="p">,</span> <span class="n">description</span><span class="p">,</span> <span class="n">fields</span><span class="p">)</span>

<span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;Question&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Text-to-SQL</strong></dt><dd><ul class="simple">
<li><p>Database description: provide LLM with accurate description of the database, such as
<code class="docutils literal notranslate"><span class="pre">CREATE</span> <span class="pre">TABLE</span></code> description for each table with column names and types, and can also
include example rows from the table</p></li>
<li><p>Few-shot examples: append standard static examples in the prompt to guide the agent
on how it should build queries based on questions</p></li>
<li><p>always run queries with a user with read-only permissions</p></li>
<li><p>database user running the queries should have access only to the necessary tables</p></li>
<li><p>add a time-out to the queries to protect from expensive query</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_community.tools.sql_database.tool</span> <span class="kn">import</span> <span class="n">QuerySQLDatabaseTool</span>
<span class="kn">from</span> <span class="nn">langchain_community.utilities</span> <span class="kn">import</span> <span class="n">SQLDatabase</span>
<span class="kn">from</span> <span class="nn">langchain.chains.sql_database.query</span> <span class="kn">import</span> <span class="n">create_sql_query_chain</span>

<span class="n">db</span> <span class="o">=</span> <span class="n">SQLDatabase</span><span class="o">.</span><span class="n">from_uri</span><span class="p">(</span><span class="n">connection</span><span class="p">)</span>

<span class="n">write_query</span> <span class="o">=</span> <span class="n">create_sql_query_chain</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">db</span><span class="p">)</span>
<span class="n">execute_query</span> <span class="o">=</span> <span class="n">QuerySQLDatabaseTool</span><span class="p">(</span><span class="n">db</span><span class="o">=</span><span class="n">db</span><span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">write_query</span> <span class="o">|</span> <span class="n">execute_query</span>
<span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s1">&#39;Question&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p><a class="reference external" href="#langchain">back to top</a></p>
</section>
</section>
<section id="langgraph">
<h2>LangGraph<a class="headerlink" href="#langgraph" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#graph">Graph</a>, <a class="reference internal" href="#memory">Memory</a>, <a class="reference internal" href="#multiactor">Multiactor</a></p></li>
</ul>
<section id="graph">
<h3>Graph<a class="headerlink" href="#graph" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>LangGraph is an open source library by LangChain</p></li>
<li><p>enable developers to implement multiactor, multistep, and stateful cognitive architectures
called graphs</p></li>
<li><p>State: data received from outside, modified and produced by the app</p></li>
<li><p>Node: Python or JavaScript functions, receiving current state and returning updated state</p></li>
<li><p>Edge: connection between nodes, can be fixed path or conditional</p></li>
<li><p>need to define the state of the graph first</p></li>
<li><p>state keys without an annotation will be overwritten</p></li>
<li><p>without explicit instruction, execution is stopped when there’s no more nodes to run</p></li>
<li><p>graph is compiled into a runnable object</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span><span class="p">,</span> <span class="n">TypedDict</span>
<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span>
<span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">END</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">StateGraph</span>
<span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>

<span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>

<span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">chatbot</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">answer</span><span class="p">]}</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;chatbot&quot;</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">)</span>

<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;chatbot&quot;</span><span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;chatbot&quot;</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>

<span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>

<span class="nb">input</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s1">&#39;hi!&#39;</span><span class="p">)]}</span>

<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</section>
<section id="memory">
<h3>Memory<a class="headerlink" href="#memory" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>LLMs are stateless, with prior prompt or model response is lost with a new response</p></li>
<li><p>including previous conversations and context in the final prompt can give memory</p></li>
<li><p>chat history can be stored as a list of messages, append recent messages after each turn,
or append into prompt by inserting the messages into the prompt</p></li>
<li><p>appending chat history in the prompt have scalability issues</p></li>
<li><p>Checkpointer: storage adapter for in-memory, SQLite, Postgres, Redis, and MySQL</p></li>
<li><p>Thread: also called interaction, auto created when first used</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="kn">from</span> <span class="nn">langchain_core.runnables.config</span> <span class="kn">import</span> <span class="n">RunnableConfig</span>

<span class="c1"># stores the state at the end of each step</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">MemorySaver</span><span class="p">())</span>

<span class="n">thread_1</span> <span class="o">=</span> <span class="n">RunnableConfig</span><span class="p">({</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">}})</span>
<span class="n">result_1</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;hi, my name is Jack!&quot;</span><span class="p">)]},</span> <span class="n">thread_1</span>
<span class="p">)</span>

<span class="n">result_2</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="s2">&quot;what is my name?&quot;</span><span class="p">)]},</span> <span class="n">thread_1</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</section>
<section id="multiactor">
<h3>Multiactor<a class="headerlink" href="#multiactor" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>application with multiple actors needs a coordination layer to define actors, hand off
work, and schedule execution of each actor</p></li>
<li><p>each actor should help update a single central state</p></li>
<li><p>with a single  central state, a snapshot can be made, execution can be paused and
human-in-the-loop control can be implemented</p></li>
</ul>
</div></blockquote>
<p><a class="reference external" href="#langchain">back to top</a></p>
</section>
</section>
<section id="prompting-basics">
<h2>Prompting Basics<a class="headerlink" href="#prompting-basics" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#llms">LLMs</a>, <a class="reference internal" href="#zero-shot-prompting">Zero-Shot Prompting</a>, <a class="reference internal" href="#few-shot-prompting">Few-Shot Prompting</a></p></li>
<li><p>prompts help the model understand context and generate relevant answers to queries</p></li>
<li><p>prompt engineering: adapting an existing LLM for specific task</p></li>
<li><p>Temperature: controls the randomness of LLM output</p></li>
<li><p>prompting techniques are most useful when combined with others</p></li>
</ul>
<section id="llms">
<h3>LLMs<a class="headerlink" href="#llms" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt><strong>Fine-Tuned</strong></dt><dd><ul>
<li><p>created by taking base LLMs, and further train on a proprietary dataset for a
specific task</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Instruction-Tuned</strong></dt><dd><ul>
<li><p>fine-tuned with task-specific datasets and RLHF</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Dialogue-Tuned</strong></dt><dd><ul>
<li><p>enhanced instruction-tuned LLMs</p></li>
<li><p>uses dialogue dataset and chat format</p></li>
<li><p>text is divided into parts associated with a role</p></li>
<li><p>System role: for instructions and framing the task</p></li>
<li><p>User role: actual task or question</p></li>
<li><p>Assistant role: for outputs of the model</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="zero-shot-prompting">
<h3>Zero-Shot Prompting<a class="headerlink" href="#zero-shot-prompting" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>simply telling the LLM to perform the desired task</p></li>
<li><p>usually work for simple questions</p></li>
<li><p>will need to iterate on prompts and responses to get a reliable system</p></li>
<li><dl class="simple">
<dt><strong>Chain-of-Thought</strong></dt><dd><ul>
<li><p>instructing the model to take time to think</p></li>
<li><p>prepending the prompt with instructions form the LLM to describe how it could arrive
at the answer</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Retrieval-Augmented Generation</strong></dt><dd><ul>
<li><p>RAG: finding relevant context, and including them in the prompt</p></li>
<li><p>should be combined with CoT</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Tool Calling</strong></dt><dd><ul>
<li><p>prepending the prompt with a list of external functions LLM can use</p></li>
<li><p>developer should parse the output, and call functions that the LLM wants to use</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<section id="few-shot-prompting">
<h3>Few-Shot Prompting<a class="headerlink" href="#few-shot-prompting" title="Link to this heading">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>providing LLM with examples of other questions and correct answers</p></li>
<li><p>enables LLM to learn how to perform a new task without going through additional training
or fine-tuning</p></li>
<li><p>less powerful than fine-tuning, but more flexible and can do it at query time</p></li>
<li><dl class="simple">
<dt><strong>Static</strong></dt><dd><ul>
<li><p>include a predetermined list of a small number of examples in the prompt</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Dynamic</strong></dt><dd><ul>
<li><p>from a dataset of many examples, choose the most relevant ones for each new query</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p><a class="reference external" href="#langchain">back to top</a></p>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Kubernetes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Kubernetes</p>
      </div>
    </a>
    <a class="right-next"
       href="Linux.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Linux</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basics">Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interfaces">Interfaces</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-output">LLM Output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#component-composition">Component Composition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rag">RAG</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-indexing">Data Indexing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing-optimisations">Indexing Optimisations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-transformation">Query Transformation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-routing">Query Routing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-construction">Query Construction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#langgraph">LangGraph</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph">Graph</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory">Memory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiactor">Multiactor</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompting-basics">Prompting Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llms">LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-prompting">Zero-Shot Prompting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#few-shot-prompting">Few-Shot Prompting</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Myat Kyaw Tun
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025, Myat Kyaw Tun.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>